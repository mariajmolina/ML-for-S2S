{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, torch, numpy as np\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "#seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "#import random\n",
    "import shutil\n",
    "import psutil\n",
    "import sklearn\n",
    "import scipy\n",
    "#import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torch\n",
    "\n",
    "#from holodecml.data import PickleReader, UpsamplingReader, XarrayReader, XarrayReaderLabels\n",
    "#from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "#import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(transforms, image):\n",
    "    im = {\"image\": image}\n",
    "    for image_transform in transforms:\n",
    "        im = image_transform(im)\n",
    "    image = im[\"image\"]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"ML-for-Derecho\")\n",
    "\n",
    "import torch_funcs\n",
    "import torch_s2s_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_negone(ds, minv, maxv):\n",
    "    return (((ds + 1) / 2) * (maxv - minv)) + minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "available_ncpus = len(psutil.Process().cpu_affinity())\n",
    "\n",
    "# Set up the GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") if not is_cuda else torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 8\n"
     ]
    }
   ],
   "source": [
    "print(device, available_ncpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"gan.yml\" #\"../config/gan.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "save_loc = conf[\"save_loc\"]\n",
    "os.makedirs(save_loc, exist_ok = True)\n",
    "os.makedirs(os.path.join(save_loc, \"images\"), exist_ok = True)\n",
    "if not os.path.isfile(os.path.join(save_loc, \"model.yml\")):\n",
    "    shutil.copyfile(config, os.path.join(save_loc, \"model.yml\"))\n",
    "\n",
    "# Trainer params\n",
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "\n",
    "epochs = conf[\"trainer\"][\"epochs\"]\n",
    "batches_per_epoch = conf[\"trainer\"][\"batches_per_epoch\"]\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "lambda_gp = conf[\"trainer\"][\"lambda_gp\"]\n",
    "mask_penalty = conf[\"trainer\"][\"mask_penalty\"]\n",
    "regression_penalty = conf[\"trainer\"][\"regression_penalty\"]\n",
    "train_gen_every = conf[\"trainer\"][\"train_gen_every\"]\n",
    "train_disc_every = conf[\"trainer\"][\"train_disc_every\"]\n",
    "threshold = conf[\"trainer\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'tas2m'\n",
    "wks = 2\n",
    "\n",
    "train = torch_s2s_dataset.S2SDataset(\n",
    "    \n",
    "    week=wks, variable=var, norm='minmax', region='fixed',\n",
    "    \n",
    "    minv=None, maxv=None, mnv=None, stdv=None,\n",
    "    \n",
    "    lon0=250., lat0=30., dxdy=32., feat_topo=True, feat_lats=True, feat_lons=True,\n",
    "    \n",
    "    startdt='1999-02-01', enddt='2015-12-31', homedir='/glade/scratch/molina/'\n",
    ")\n",
    "\n",
    "valid = torch_s2s_dataset.S2SDataset(\n",
    "    \n",
    "    week=wks, variable=var, norm='minmax', region='fixed',\n",
    "    \n",
    "    minv=train.min_val, maxv=train.max_val, mnv=None, stdv=None,\n",
    "    \n",
    "    lon0=250., lat0=30., dxdy=32., feat_topo=True, feat_lats=True, feat_lons=True,\n",
    "    \n",
    "    startdt='2016-01-01', enddt='2017-12-31', homedir='/glade/scratch/molina/'\n",
    ")\n",
    "\n",
    "tests = torch_s2s_dataset.S2SDataset(\n",
    "    \n",
    "    week=wks, variable=var, norm='minmax', region='fixed',\n",
    "    \n",
    "    minv=train.min_val, maxv=train.max_val, mnv=None, stdv=None,\n",
    "    \n",
    "    lon0=250., lat0=30., dxdy=32., feat_topo=True, feat_lats=True, feat_lons=True,\n",
    "    \n",
    "    startdt='2018-01-01', enddt='2020-12-31', homedir='/glade/scratch/molina/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train,\n",
    "#     batch_size=train_batch_size,\n",
    "#     num_workers=available_ncpus//2,\n",
    "#     pin_memory=True,\n",
    "#     shuffle=True)\n",
    "\n",
    "# valid_loader = torch.utils.data.DataLoader(\n",
    "#     valid,\n",
    "#     batch_size=train_batch_size,\n",
    "#     num_workers=available_ncpus//2,\n",
    "#     pin_memory=True,\n",
    "#     shuffle=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid, batch_size=train_batch_size, shuffle=False, drop_last=False)\n",
    "tests_loader = DataLoader(tests, batch_size=train_batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model(conf[\"generator\"]).to(device) \n",
    "discriminator = load_model(conf[\"discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=4,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /glade/work/schreck/py37/lib/python3.7/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "if adv_loss == \"bce\":\n",
    "    adversarial_loss = torch.nn.BCELoss().to(device)\n",
    "    \n",
    "perceptual_alex = lpips.LPIPS(net='alex').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "    lr = conf[\"optimizer_G\"][\"learning_rate\"],\n",
    "    betas = (conf[\"optimizer_G\"][\"b0\"], conf[\"optimizer_G\"][\"b1\"]))\n",
    "\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, discriminator.parameters()), \n",
    "    lr = conf[\"optimizer_D\"][\"learning_rate\"], \n",
    "    betas = (conf[\"optimizer_D\"][\"b0\"], conf[\"optimizer_D\"][\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_imgs, gen_imgs):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "    interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "    out = discriminator(interpolated)[1]\n",
    "    grad = torch.autograd.grad(outputs=out,\n",
    "                               inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                               retain_graph=True,\n",
    "                               create_graph=True,\n",
    "                               only_inputs=True)[0]\n",
    "    grad = grad.view(grad.size(0), -1)\n",
    "    grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "    d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "    return d_loss_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_G_decay = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=30, gamma=0.2)\n",
    "lr_D_decay = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=30, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.234433 p_real 0.2542 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [01:02<02:48,  2.30s/it]\n",
      "Epoch 1 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.093711 p_real 0.1116 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:59<02:39,  2.19s/it]\n",
      "Epoch 2 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.070027 p_real 0.0721 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:59<02:40,  2.19s/it]\n",
      "Epoch 3 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.062675 p_real 0.0489 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:59<02:41,  2.22s/it]\n",
      "Epoch 4 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.060761 p_real 0.0381 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:59<02:40,  2.20s/it]\n",
      "Epoch 5 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.064831 p_real 0.0428 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:58<02:37,  2.16s/it]\n",
      "Epoch 6 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.047884 p_real 0.0310 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:51<02:19,  1.92s/it]\n",
      "Epoch 7 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.043514 p_real 0.0200 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:42<01:53,  1.56s/it]\n",
      "Epoch 8 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.039293 p_real 0.0148 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:40<01:48,  1.48s/it]\n",
      "Epoch 9 D_loss nan D_reg nan G_loss    nan G_reg    nan G_mae 0.038279 p_real 0.0120 real_acc nan fake_acc nan:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 10 D_loss 0.014108 D_reg 124890.036458 G_loss -0.685418 G_reg    nan G_mae 0.044550 p_real 0.0265 real_acc 0.7708 fake_acc 0.1979:  27%|██▋       | 27/100 [00:46<02:06,  1.73s/it]\n",
      "Epoch 11 D_loss 0.001724 D_reg 84903.114583 G_loss -0.692062 G_reg    nan G_mae 0.042150 p_real 0.0293 real_acc 0.7500 fake_acc 0.2083:  27%|██▋       | 27/100 [00:46<02:06,  1.73s/it]  \n",
      "Epoch 12 D_loss 0.032317 D_reg 70639.324219 G_loss -0.705527 G_reg    nan G_mae 0.041193 p_real 0.0243 real_acc 0.6979 fake_acc 0.1875:  27%|██▋       | 27/100 [00:45<02:02,  1.68s/it] \n",
      "Epoch 13 D_loss -0.000117 D_reg 43351.051432 G_loss -0.703379 G_reg    nan G_mae 0.039177 p_real 0.0213 real_acc 0.7292 fake_acc 0.2500:  27%|██▋       | 27/100 [00:46<02:06,  1.73s/it]\n",
      "Epoch 14 D_loss 0.001331 D_reg 146188.598958 G_loss -0.700960 G_reg    nan G_mae 0.038423 p_real 0.0176 real_acc 0.7500 fake_acc 0.2083:  27%|██▋       | 27/100 [00:49<02:13,  1.83s/it] \n",
      "Epoch 15 D_loss 0.003158 D_reg 79460.579427 G_loss -0.731808 G_reg    nan G_mae 0.038482 p_real 0.0173 real_acc 0.7812 fake_acc 0.1875:  27%|██▋       | 27/100 [00:51<02:20,  1.92s/it]\n",
      "Epoch 16 D_loss 0.042785 D_reg 85743.919271 G_loss -0.741155 G_reg    nan G_mae 0.038903 p_real 0.0180 real_acc 0.7500 fake_acc 0.1562:  27%|██▋       | 27/100 [00:53<02:25,  2.00s/it]\n",
      "Epoch 17 D_loss 0.035528 D_reg 25999.218750 G_loss -0.745335 G_reg    nan G_mae 0.039679 p_real 0.0155 real_acc 0.8333 fake_acc 0.1458:  27%|██▋       | 27/100 [00:57<02:34,  2.11s/it]\n",
      "Epoch 18 D_loss 0.015106 D_reg 17567.055990 G_loss -0.759456 G_reg    nan G_mae 0.034871 p_real 0.0107 real_acc 0.8229 fake_acc 0.1250:  27%|██▋       | 27/100 [00:50<02:16,  1.87s/it] \n",
      "Epoch 19 D_loss -0.008273 D_reg 44046.467448 G_loss -0.753908 G_reg    nan G_mae 0.038838 p_real 0.0096 real_acc 0.8333 fake_acc 0.1771:  27%|██▋       | 27/100 [00:49<02:12,  1.82s/it]\n",
      "Epoch 20 D_loss 0.034660 D_reg 30022.241536 G_loss -0.765149 G_reg    nan G_mae 0.036911 p_real 0.0094 real_acc 0.8125 fake_acc 0.1458:  27%|██▋       | 27/100 [00:45<02:01,  1.67s/it]\n",
      "Epoch 21 D_loss 0.004990 D_reg 48829.066406 G_loss -0.751224 G_reg    nan G_mae 0.033628 p_real 0.0087 real_acc 0.8333 fake_acc 0.1562:  27%|██▋       | 27/100 [00:48<02:12,  1.81s/it]\n",
      "Epoch 22 D_loss 0.014963 D_reg 33546.923828 G_loss -0.745055 G_reg    nan G_mae 0.032300 p_real 0.0074 real_acc 0.8333 fake_acc 0.1562:  27%|██▋       | 27/100 [00:40<01:50,  1.51s/it] \n",
      "Epoch 23 D_loss 0.019064 D_reg 20851.888997 G_loss -0.755275 G_reg    nan G_mae 0.033979 p_real 0.0086 real_acc 0.8438 fake_acc 0.1562:  27%|██▋       | 27/100 [00:49<02:13,  1.83s/it] \n",
      "Epoch 24 D_loss -0.006133 D_reg 40926.492839 G_loss -0.772680 G_reg    nan G_mae 0.033942 p_real 0.0077 real_acc 0.8438 fake_acc 0.1146:  27%|██▋       | 27/100 [00:48<02:11,  1.80s/it]\n",
      "Epoch 25 D_loss -0.001562 D_reg 30739.512370 G_loss -0.746801 G_reg    nan G_mae 0.036086 p_real 0.0064 real_acc 0.8333 fake_acc 0.1667:  27%|██▋       | 27/100 [00:50<02:16,  1.87s/it]\n",
      "Epoch 26 D_loss -0.003712 D_reg 30972.547526 G_loss -0.747714 G_reg    nan G_mae 0.034796 p_real 0.0063 real_acc 0.9062 fake_acc 0.1354:  27%|██▋       | 27/100 [00:46<02:06,  1.73s/it]\n",
      "Epoch 27 D_loss -0.000544 D_reg 55218.270833 G_loss -0.767548 G_reg    nan G_mae 0.032746 p_real 0.0066 real_acc 0.8854 fake_acc 0.1667:  27%|██▋       | 27/100 [00:41<01:53,  1.55s/it]\n",
      "Epoch 28 D_loss 0.046556 D_reg 13822.723958 G_loss -0.781970 G_reg    nan G_mae 0.035197 p_real 0.0062 real_acc 0.8542 fake_acc 0.1042:  27%|██▋       | 27/100 [00:39<01:46,  1.47s/it]\n",
      "Epoch 29 D_loss -0.014646 D_reg 40400.462240 G_loss -0.781494 G_reg    nan G_mae 0.032423 p_real 0.0053 real_acc 0.9167 fake_acc 0.0938:  27%|██▋       | 27/100 [00:38<01:44,  1.43s/it]\n",
      "Epoch 30 D_loss 0.007364 D_reg 30185.671875 G_loss -0.808855 G_reg    nan G_mae 0.031059 p_real 0.0046 real_acc 0.8854 fake_acc 0.0625:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it]\n",
      "Epoch 31 D_loss 0.032139 D_reg 20338.084310 G_loss -0.815915 G_reg    nan G_mae 0.028342 p_real 0.0047 real_acc 0.9062 fake_acc 0.0625:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it]\n",
      "Epoch 32 D_loss 0.028119 D_reg 27947.179688 G_loss -0.811116 G_reg    nan G_mae 0.027048 p_real 0.0042 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:45,  1.44s/it]\n",
      "Epoch 33 D_loss 0.032622 D_reg 23326.854167 G_loss -0.803473 G_reg    nan G_mae 0.027516 p_real 0.0041 real_acc 0.8958 fake_acc 0.0729:  27%|██▋       | 27/100 [00:37<01:42,  1.41s/it]\n",
      "Epoch 34 D_loss 0.015155 D_reg 15902.786133 G_loss -0.803307 G_reg    nan G_mae 0.028511 p_real 0.0044 real_acc 0.8958 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 35 D_loss 0.020887 D_reg 16489.832031 G_loss -0.815971 G_reg    nan G_mae 0.026840 p_real 0.0041 real_acc 0.8958 fake_acc 0.0729:  27%|██▋       | 27/100 [00:38<01:42,  1.41s/it]\n",
      "Epoch 36 D_loss 0.054075 D_reg 29438.502279 G_loss -0.811415 G_reg    nan G_mae 0.027383 p_real 0.0041 real_acc 0.8750 fake_acc 0.0729:  27%|██▋       | 27/100 [00:39<01:47,  1.48s/it]\n",
      "Epoch 37 D_loss 0.023073 D_reg 25637.941406 G_loss -0.812094 G_reg    nan G_mae 0.026551 p_real 0.0040 real_acc 0.9062 fake_acc 0.0729:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 38 D_loss 0.021295 D_reg 25813.333984 G_loss -0.815729 G_reg    nan G_mae 0.026673 p_real 0.0039 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 39 D_loss 0.030834 D_reg 16453.495117 G_loss -0.818887 G_reg    nan G_mae 0.027062 p_real 0.0037 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it] \n",
      "Epoch 40 D_loss 0.068894 D_reg 21930.658854 G_loss -0.826022 G_reg    nan G_mae 0.027651 p_real 0.0038 real_acc 0.8438 fake_acc 0.0521:  27%|██▋       | 27/100 [00:40<01:48,  1.49s/it]\n",
      "Epoch 41 D_loss 0.029500 D_reg 19969.242513 G_loss -0.826000 G_reg    nan G_mae 0.026852 p_real 0.0039 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:48,  1.48s/it]\n",
      "Epoch 42 D_loss 0.033169 D_reg 20051.980469 G_loss -0.825315 G_reg    nan G_mae 0.028207 p_real 0.0039 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 43 D_loss 0.024118 D_reg 18597.297689 G_loss -0.828070 G_reg    nan G_mae 0.025958 p_real 0.0037 real_acc 0.9062 fake_acc 0.0729:  27%|██▋       | 27/100 [00:38<01:44,  1.44s/it]\n",
      "Epoch 44 D_loss 0.029537 D_reg 12428.855143 G_loss -0.827039 G_reg    nan G_mae 0.026669 p_real 0.0039 real_acc 0.8750 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 45 D_loss 0.015802 D_reg 11821.284831 G_loss -0.823055 G_reg    nan G_mae 0.027715 p_real 0.0039 real_acc 0.8958 fake_acc 0.0833:  27%|██▋       | 27/100 [00:40<01:50,  1.51s/it]\n",
      "Epoch 46 D_loss 0.038055 D_reg 42283.375000 G_loss -0.832070 G_reg    nan G_mae 0.025908 p_real 0.0038 real_acc 0.8958 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 47 D_loss 0.058355 D_reg 25475.667969 G_loss -0.841683 G_reg    nan G_mae 0.025577 p_real 0.0038 real_acc 0.8958 fake_acc 0.0625:  27%|██▋       | 27/100 [00:39<01:47,  1.48s/it]\n",
      "Epoch 48 D_loss 0.032345 D_reg 35092.244792 G_loss -0.832232 G_reg    nan G_mae 0.027867 p_real 0.0040 real_acc 0.9375 fake_acc 0.0625:  27%|██▋       | 27/100 [00:46<02:04,  1.71s/it]\n",
      "Epoch 49 D_loss 0.006263 D_reg 15124.461833 G_loss -0.828210 G_reg    nan G_mae 0.029160 p_real 0.0043 real_acc 0.9375 fake_acc 0.0833:  27%|██▋       | 27/100 [00:42<01:55,  1.58s/it]\n",
      "Epoch 50 D_loss 0.024781 D_reg 11876.339681 G_loss -0.831742 G_reg    nan G_mae 0.027562 p_real 0.0039 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:47<02:07,  1.75s/it]\n",
      "Epoch 51 D_loss 0.040164 D_reg 26613.716146 G_loss -0.832499 G_reg    nan G_mae 0.027651 p_real 0.0039 real_acc 0.9062 fake_acc 0.0625:  27%|██▋       | 27/100 [00:49<02:14,  1.84s/it]\n",
      "Epoch 52 D_loss 0.013873 D_reg 23537.987630 G_loss -0.833840 G_reg    nan G_mae 0.025957 p_real 0.0038 real_acc 0.9167 fake_acc 0.0833:  27%|██▋       | 27/100 [00:46<02:06,  1.73s/it]\n",
      "Epoch 53 D_loss 0.040945 D_reg 13710.008464 G_loss -0.837331 G_reg    nan G_mae 0.025896 p_real 0.0040 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:42<01:54,  1.57s/it]\n",
      "Epoch 54 D_loss 0.025272 D_reg 19182.559896 G_loss -0.832700 G_reg    nan G_mae 0.027119 p_real 0.0039 real_acc 0.9167 fake_acc 0.0729:  27%|██▋       | 27/100 [00:40<01:49,  1.51s/it]\n",
      "Epoch 55 D_loss 0.016583 D_reg 10153.894531 G_loss -0.837871 G_reg    nan G_mae 0.026666 p_real 0.0040 real_acc 0.9167 fake_acc 0.0833:  27%|██▋       | 27/100 [00:42<01:55,  1.59s/it]\n",
      "Epoch 56 D_loss 0.006635 D_reg 11978.408854 G_loss -0.835402 G_reg    nan G_mae 0.026498 p_real 0.0040 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:45<02:02,  1.67s/it]\n",
      "Epoch 57 D_loss 0.016228 D_reg 12038.654134 G_loss -0.832796 G_reg    nan G_mae 0.026943 p_real 0.0038 real_acc 0.9271 fake_acc 0.0729:  27%|██▋       | 27/100 [00:46<02:05,  1.72s/it]\n",
      "Epoch 58 D_loss -0.010715 D_reg 14341.659180 G_loss -0.835175 G_reg    nan G_mae 0.025978 p_real 0.0038 real_acc 0.9167 fake_acc 0.0833:  27%|██▋       | 27/100 [00:40<01:49,  1.51s/it]\n",
      "Epoch 59 D_loss 0.043248 D_reg 26164.844727 G_loss -0.839254 G_reg    nan G_mae 0.025358 p_real 0.0039 real_acc 0.8646 fake_acc 0.0625:  27%|██▋       | 27/100 [00:43<01:56,  1.59s/it]\n",
      "Epoch 60 D_loss 0.019670 D_reg 18983.681641 G_loss -0.830873 G_reg    nan G_mae 0.026154 p_real 0.0039 real_acc 0.8958 fake_acc 0.0625:  27%|██▋       | 27/100 [00:49<02:12,  1.82s/it] \n",
      "Epoch 61 D_loss 0.033904 D_reg 11930.555990 G_loss -0.833854 G_reg    nan G_mae 0.024540 p_real 0.0040 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:54<02:27,  2.03s/it] \n",
      "Epoch 62 D_loss 0.030287 D_reg 12270.711100 G_loss -0.832945 G_reg    nan G_mae 0.024757 p_real 0.0038 real_acc 0.9271 fake_acc 0.0521:  27%|██▋       | 27/100 [00:58<02:36,  2.15s/it]\n",
      "Epoch 63 D_loss 0.031379 D_reg 16139.417643 G_loss -0.834302 G_reg    nan G_mae 0.026188 p_real 0.0039 real_acc 0.8958 fake_acc 0.0417:  27%|██▋       | 27/100 [00:54<02:26,  2.01s/it]\n",
      "Epoch 64 D_loss 0.028415 D_reg 39450.432943 G_loss -0.829802 G_reg    nan G_mae 0.025801 p_real 0.0039 real_acc 0.9271 fake_acc 0.0521:  27%|██▋       | 27/100 [00:51<02:20,  1.92s/it]\n",
      "Epoch 65 D_loss 0.045336 D_reg 22914.452799 G_loss -0.832529 G_reg    nan G_mae 0.025693 p_real 0.0038 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:47<02:07,  1.75s/it]\n",
      "Epoch 66 D_loss 0.013619 D_reg 18459.600586 G_loss -0.832737 G_reg    nan G_mae 0.025247 p_real 0.0038 real_acc 0.8958 fake_acc 0.0833:  27%|██▋       | 27/100 [00:44<02:01,  1.66s/it]\n",
      "Epoch 67 D_loss 0.016724 D_reg 11093.328125 G_loss -0.837801 G_reg    nan G_mae 0.024934 p_real 0.0039 real_acc 0.9583 fake_acc 0.0729:  27%|██▋       | 27/100 [00:44<01:59,  1.64s/it]\n",
      "Epoch 68 D_loss 0.034654 D_reg 15228.961263 G_loss -0.841626 G_reg    nan G_mae 0.026046 p_real 0.0038 real_acc 0.9479 fake_acc 0.0625:  27%|██▋       | 27/100 [00:45<02:01,  1.67s/it]\n",
      "Epoch 69 D_loss 0.000572 D_reg 18933.656901 G_loss -0.835998 G_reg    nan G_mae 0.025417 p_real 0.0039 real_acc 0.9688 fake_acc 0.0729:  27%|██▋       | 27/100 [00:45<02:03,  1.69s/it]\n",
      "Epoch 70 D_loss 0.032054 D_reg 10403.668132 G_loss -0.834093 G_reg    nan G_mae 0.025427 p_real 0.0039 real_acc 0.9583 fake_acc 0.0521:  27%|██▋       | 27/100 [00:44<02:00,  1.66s/it]\n",
      "Epoch 71 D_loss 0.047711 D_reg 15126.530924 G_loss -0.833542 G_reg    nan G_mae 0.025086 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:42<01:55,  1.59s/it]\n",
      "Epoch 72 D_loss 0.026184 D_reg 13289.031901 G_loss -0.835662 G_reg    nan G_mae 0.028120 p_real 0.0040 real_acc 0.9271 fake_acc 0.0729:  27%|██▋       | 27/100 [00:48<02:12,  1.81s/it]\n",
      "Epoch 73 D_loss 0.046360 D_reg 14977.515625 G_loss -0.833345 G_reg    nan G_mae 0.025861 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:50<02:17,  1.88s/it]\n",
      "Epoch 74 D_loss 0.028787 D_reg 19734.614746 G_loss -0.837204 G_reg    nan G_mae 0.026240 p_real 0.0040 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:53<02:24,  1.97s/it]\n",
      "Epoch 75 D_loss 0.054782 D_reg 11803.354818 G_loss -0.836741 G_reg    nan G_mae 0.027075 p_real 0.0039 real_acc 0.9167 fake_acc 0.0729:  27%|██▋       | 27/100 [00:55<02:30,  2.06s/it]\n",
      "Epoch 76 D_loss 0.032498 D_reg 14819.183919 G_loss -0.832904 G_reg    nan G_mae 0.025357 p_real 0.0038 real_acc 0.8958 fake_acc 0.0521:  27%|██▋       | 27/100 [00:50<02:17,  1.88s/it]\n",
      "Epoch 77 D_loss 0.023547 D_reg 33991.974121 G_loss -0.837778 G_reg    nan G_mae 0.025124 p_real 0.0039 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:47<02:09,  1.77s/it]\n",
      "Epoch 78 D_loss 0.003480 D_reg 11707.735026 G_loss -0.834840 G_reg    nan G_mae 0.025145 p_real 0.0040 real_acc 0.9583 fake_acc 0.0417:  27%|██▋       | 27/100 [00:48<02:11,  1.80s/it]\n",
      "Epoch 79 D_loss 0.041209 D_reg 11852.933105 G_loss -0.834544 G_reg    nan G_mae 0.025814 p_real 0.0040 real_acc 0.8646 fake_acc 0.0312:  27%|██▋       | 27/100 [00:47<02:09,  1.77s/it]\n",
      "Epoch 80 D_loss 0.014321 D_reg 12914.954264 G_loss -0.841160 G_reg    nan G_mae 0.026218 p_real 0.0039 real_acc 0.9375 fake_acc 0.0729:  27%|██▋       | 27/100 [00:45<02:04,  1.70s/it] \n",
      "Epoch 81 D_loss 0.035693 D_reg 18794.181641 G_loss -0.843982 G_reg    nan G_mae 0.025336 p_real 0.0039 real_acc 0.8854 fake_acc 0.0625:  27%|██▋       | 27/100 [00:45<02:03,  1.69s/it]\n",
      "Epoch 82 D_loss 0.041718 D_reg 13564.381185 G_loss -0.844024 G_reg    nan G_mae 0.025452 p_real 0.0039 real_acc 0.9271 fake_acc 0.0312:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 83 D_loss 0.043771 D_reg 15301.393555 G_loss -0.839148 G_reg    nan G_mae 0.025898 p_real 0.0041 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:43<01:58,  1.62s/it]\n",
      "Epoch 84 D_loss 0.027078 D_reg 9495.194010 G_loss -0.835065 G_reg    nan G_mae 0.026399 p_real 0.0040 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:46<02:04,  1.71s/it] \n",
      "Epoch 85 D_loss 0.028830 D_reg 19742.073568 G_loss -0.846810 G_reg    nan G_mae 0.025457 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:47<02:09,  1.77s/it]\n",
      "Epoch 86 D_loss 0.051665 D_reg 22406.136882 G_loss -0.841691 G_reg    nan G_mae 0.025321 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:51<02:18,  1.90s/it]\n",
      "Epoch 87 D_loss 0.025182 D_reg 19252.923503 G_loss -0.832544 G_reg    nan G_mae 0.025092 p_real 0.0038 real_acc 0.9167 fake_acc 0.0625:  27%|██▋       | 27/100 [00:50<02:17,  1.88s/it]\n",
      "Epoch 88 D_loss 0.021337 D_reg 10960.783854 G_loss -0.833632 G_reg    nan G_mae 0.026965 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:50<02:16,  1.86s/it]\n",
      "Epoch 89 D_loss 0.038110 D_reg 14282.819661 G_loss -0.842591 G_reg    nan G_mae 0.025766 p_real 0.0038 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:44<02:01,  1.66s/it]\n",
      "Epoch 90 D_loss 0.036885 D_reg 12130.407552 G_loss -0.842311 G_reg    nan G_mae 0.025926 p_real 0.0039 real_acc 0.9271 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:43,  1.41s/it]\n",
      "Epoch 91 D_loss 0.045475 D_reg 23408.316406 G_loss -0.839440 G_reg    nan G_mae 0.024808 p_real 0.0038 real_acc 0.9062 fake_acc 0.0625:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it]\n",
      "Epoch 92 D_loss 0.017518 D_reg 10298.002604 G_loss -0.838426 G_reg    nan G_mae 0.025278 p_real 0.0039 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 93 D_loss 0.046291 D_reg 34150.818034 G_loss -0.838272 G_reg    nan G_mae 0.024757 p_real 0.0038 real_acc 0.9271 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 94 D_loss 0.027163 D_reg 19428.585286 G_loss -0.837234 G_reg    nan G_mae 0.024822 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:45,  1.45s/it]\n",
      "Epoch 95 D_loss 0.032714 D_reg 18536.635742 G_loss -0.837275 G_reg    nan G_mae 0.024662 p_real 0.0039 real_acc 0.9375 fake_acc 0.0625:  27%|██▋       | 27/100 [00:41<01:52,  1.54s/it]\n",
      "Epoch 96 D_loss 0.031362 D_reg 6533.849447 G_loss -0.844766 G_reg    nan G_mae 0.024656 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:51,  1.52s/it]\n",
      "Epoch 97 D_loss 0.022679 D_reg 9058.247396 G_loss -0.842657 G_reg    nan G_mae 0.024739 p_real 0.0039 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:44,  1.43s/it] \n",
      "Epoch 98 D_loss 0.028662 D_reg 29303.060221 G_loss -0.845966 G_reg    nan G_mae 0.025116 p_real 0.0039 real_acc 0.9271 fake_acc 0.0521:  27%|██▋       | 27/100 [00:40<01:50,  1.51s/it]\n",
      "Epoch 99 D_loss 0.031051 D_reg 12528.324382 G_loss -0.838535 G_reg    nan G_mae 0.024817 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 100 D_loss 0.050399 D_reg 16791.579753 G_loss -0.842717 G_reg    nan G_mae 0.024888 p_real 0.0038 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:41<01:52,  1.54s/it]\n",
      "Epoch 101 D_loss 0.030469 D_reg 20230.169108 G_loss -0.846637 G_reg    nan G_mae 0.025841 p_real 0.0038 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:53,  1.55s/it]\n",
      "Epoch 102 D_loss 0.023329 D_reg 17971.322754 G_loss -0.836836 G_reg    nan G_mae 0.026052 p_real 0.0040 real_acc 0.9688 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:48,  1.48s/it]\n",
      "Epoch 103 D_loss 0.074805 D_reg 15937.043620 G_loss -0.843099 G_reg    nan G_mae 0.025946 p_real 0.0039 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:53,  1.55s/it]\n",
      "Epoch 104 D_loss 0.030257 D_reg 15878.103190 G_loss -0.840718 G_reg    nan G_mae 0.026323 p_real 0.0040 real_acc 0.8854 fake_acc 0.0521:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 105 D_loss 0.036987 D_reg 31031.860026 G_loss -0.835529 G_reg    nan G_mae 0.023992 p_real 0.0039 real_acc 0.9167 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 106 D_loss 0.043438 D_reg 12306.572266 G_loss -0.848653 G_reg    nan G_mae 0.023715 p_real 0.0037 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:52,  1.53s/it]\n",
      "Epoch 107 D_loss 0.015768 D_reg 35289.429362 G_loss -0.837492 G_reg    nan G_mae 0.025579 p_real 0.0039 real_acc 0.9271 fake_acc 0.0625:  27%|██▋       | 27/100 [00:41<01:50,  1.52s/it] \n",
      "Epoch 108 D_loss 0.050866 D_reg 9234.983724 G_loss -0.844258 G_reg    nan G_mae 0.024699 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:50,  1.52s/it]\n",
      "Epoch 109 D_loss 0.048327 D_reg 27103.139323 G_loss -0.839665 G_reg    nan G_mae 0.026192 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 110 D_loss 0.029644 D_reg 7841.640625 G_loss -0.841160 G_reg    nan G_mae 0.025543 p_real 0.0039 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:43<01:58,  1.62s/it] \n",
      "Epoch 111 D_loss 0.028715 D_reg 20856.075521 G_loss -0.837130 G_reg    nan G_mae 0.025748 p_real 0.0040 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:51,  1.52s/it]\n",
      "Epoch 112 D_loss 0.018906 D_reg 46109.762695 G_loss -0.840336 G_reg    nan G_mae 0.024463 p_real 0.0039 real_acc 0.9271 fake_acc 0.0729:  27%|██▋       | 27/100 [00:46<02:05,  1.73s/it]\n",
      "Epoch 113 D_loss 0.026437 D_reg 12469.262695 G_loss -0.840408 G_reg    nan G_mae 0.027639 p_real 0.0040 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:42<01:55,  1.58s/it] \n",
      "Epoch 114 D_loss 0.032886 D_reg 24298.697266 G_loss -0.837522 G_reg    nan G_mae 0.025821 p_real 0.0039 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:51,  1.53s/it]\n",
      "Epoch 115 D_loss 0.022683 D_reg 9796.583659 G_loss -0.838248 G_reg    nan G_mae 0.026441 p_real 0.0038 real_acc 0.9271 fake_acc 0.0312:  27%|██▋       | 27/100 [00:43<01:57,  1.61s/it]\n",
      "Epoch 116 D_loss 0.015383 D_reg 11721.113607 G_loss -0.839414 G_reg    nan G_mae 0.026043 p_real 0.0040 real_acc 0.9688 fake_acc 0.0208:  27%|██▋       | 27/100 [00:44<02:00,  1.66s/it]\n",
      "Epoch 117 D_loss 0.023072 D_reg 16936.395182 G_loss -0.838868 G_reg    nan G_mae 0.024973 p_real 0.0039 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:44<02:00,  1.65s/it]\n",
      "Epoch 118 D_loss 0.056922 D_reg 15162.301758 G_loss -0.835486 G_reg    nan G_mae 0.025279 p_real 0.0039 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:44<02:00,  1.65s/it]\n",
      "Epoch 119 D_loss 0.034340 D_reg 10507.036133 G_loss -0.848923 G_reg    nan G_mae 0.024913 p_real 0.0038 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:47<02:08,  1.76s/it]\n",
      "Epoch 120 D_loss 0.038492 D_reg 15878.349284 G_loss -0.846806 G_reg    nan G_mae 0.025537 p_real 0.0039 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:49<02:13,  1.83s/it]\n",
      "Epoch 121 D_loss 0.028400 D_reg 13807.979167 G_loss -0.837765 G_reg    nan G_mae 0.024156 p_real 0.0038 real_acc 0.9167 fake_acc 0.0729:  27%|██▋       | 27/100 [00:43<01:58,  1.63s/it]\n",
      "Epoch 122 D_loss 0.032553 D_reg 47754.090495 G_loss -0.845452 G_reg    nan G_mae 0.025470 p_real 0.0038 real_acc 0.9167 fake_acc 0.0312:  27%|██▋       | 27/100 [00:44<01:59,  1.63s/it]\n",
      "Epoch 123 D_loss 0.041695 D_reg 9250.647949 G_loss -0.837488 G_reg    nan G_mae 0.024614 p_real 0.0038 real_acc 0.8854 fake_acc 0.0312:  27%|██▋       | 27/100 [00:47<02:09,  1.77s/it]\n",
      "Epoch 124 D_loss 0.040104 D_reg 11165.880534 G_loss -0.841518 G_reg    nan G_mae 0.025859 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:43<01:58,  1.63s/it]\n",
      "Epoch 125 D_loss 0.044156 D_reg 8553.942220 G_loss -0.841036 G_reg    nan G_mae 0.026217 p_real 0.0039 real_acc 0.9167 fake_acc 0.0625:  27%|██▋       | 27/100 [00:46<02:05,  1.72s/it] \n",
      "Epoch 126 D_loss 0.054351 D_reg 31656.910156 G_loss -0.840998 G_reg    nan G_mae 0.024903 p_real 0.0039 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:44<01:59,  1.63s/it]\n",
      "Epoch 127 D_loss 0.047583 D_reg 19433.409505 G_loss -0.833824 G_reg    nan G_mae 0.025826 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:45<02:03,  1.69s/it]\n",
      "Epoch 128 D_loss 0.037453 D_reg 7654.378906 G_loss -0.843080 G_reg    nan G_mae 0.025090 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:44<01:59,  1.63s/it] \n",
      "Epoch 129 D_loss 0.046167 D_reg 11546.473063 G_loss -0.837440 G_reg    nan G_mae 0.025318 p_real 0.0040 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 130 D_loss 0.014469 D_reg 12046.633464 G_loss -0.843694 G_reg    nan G_mae 0.024669 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 131 D_loss 0.004477 D_reg 14674.502279 G_loss -0.839675 G_reg    nan G_mae 0.027121 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:46<02:04,  1.71s/it]\n",
      "Epoch 132 D_loss 0.012574 D_reg 15373.989095 G_loss -0.834908 G_reg    nan G_mae 0.025067 p_real 0.0040 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:45<02:01,  1.67s/it]\n",
      "Epoch 133 D_loss 0.049216 D_reg 9938.092611 G_loss -0.843557 G_reg    nan G_mae 0.024533 p_real 0.0039 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:45<02:02,  1.68s/it] \n",
      "Epoch 134 D_loss 0.038118 D_reg 7355.497396 G_loss -0.837544 G_reg    nan G_mae 0.025351 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:43<01:58,  1.62s/it]\n",
      "Epoch 135 D_loss 0.035540 D_reg 18488.805501 G_loss -0.844425 G_reg    nan G_mae 0.024700 p_real 0.0039 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:42<01:54,  1.57s/it]\n",
      "Epoch 136 D_loss 0.016812 D_reg 16233.955729 G_loss -0.838278 G_reg    nan G_mae 0.024554 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:46<02:05,  1.72s/it] \n",
      "Epoch 137 D_loss 0.002515 D_reg 11711.954753 G_loss -0.844797 G_reg    nan G_mae 0.024968 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:45<02:02,  1.68s/it] \n",
      "Epoch 138 D_loss 0.041484 D_reg 11402.421875 G_loss -0.846685 G_reg    nan G_mae 0.024647 p_real 0.0038 real_acc 0.9271 fake_acc 0.0312:  27%|██▋       | 27/100 [00:42<01:55,  1.58s/it]\n",
      "Epoch 139 D_loss 0.037926 D_reg 18961.719401 G_loss -0.844513 G_reg    nan G_mae 0.024431 p_real 0.0040 real_acc 0.8854 fake_acc 0.0625:  27%|██▋       | 27/100 [00:42<01:55,  1.58s/it]\n",
      "Epoch 140 D_loss 0.039888 D_reg 9103.185710 G_loss -0.833540 G_reg    nan G_mae 0.026623 p_real 0.0039 real_acc 0.9583 fake_acc 0.0417:  27%|██▋       | 27/100 [00:45<02:03,  1.69s/it] \n",
      "Epoch 141 D_loss 0.036500 D_reg 7937.533691 G_loss -0.840352 G_reg    nan G_mae 0.024934 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:44<02:00,  1.65s/it]\n",
      "Epoch 142 D_loss -0.001910 D_reg 38082.848470 G_loss -0.838452 G_reg    nan G_mae 0.024881 p_real 0.0038 real_acc 0.9583 fake_acc 0.0625:  27%|██▋       | 27/100 [00:46<02:05,  1.72s/it]\n",
      "Epoch 143 D_loss 0.007798 D_reg 9175.597005 G_loss -0.843674 G_reg    nan G_mae 0.024278 p_real 0.0038 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 144 D_loss 0.014442 D_reg 12005.716471 G_loss -0.838118 G_reg    nan G_mae 0.025644 p_real 0.0040 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:47<02:07,  1.74s/it]\n",
      "Epoch 145 D_loss 0.028219 D_reg 18343.386393 G_loss -0.843891 G_reg    nan G_mae 0.026008 p_real 0.0040 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:45<02:03,  1.69s/it]\n",
      "Epoch 146 D_loss 0.043358 D_reg 16780.248698 G_loss -0.840427 G_reg    nan G_mae 0.025073 p_real 0.0039 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:46<02:04,  1.71s/it]\n",
      "Epoch 147 D_loss 0.023844 D_reg 17411.901042 G_loss -0.839746 G_reg    nan G_mae 0.024896 p_real 0.0040 real_acc 0.9271 fake_acc 0.0312:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 148 D_loss 0.050810 D_reg 14456.617188 G_loss -0.838626 G_reg    nan G_mae 0.027208 p_real 0.0039 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:52,  1.54s/it]\n",
      "Epoch 149 D_loss 0.014624 D_reg 30247.500651 G_loss -0.836598 G_reg    nan G_mae 0.026171 p_real 0.0039 real_acc 0.9167 fake_acc 0.0729:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 150 D_loss 0.049948 D_reg 10987.760905 G_loss -0.841435 G_reg    nan G_mae 0.025715 p_real 0.0038 real_acc 0.9583 fake_acc 0.0312:  27%|██▋       | 27/100 [00:42<01:54,  1.57s/it]\n",
      "Epoch 151 D_loss 0.016827 D_reg 28694.136719 G_loss -0.842110 G_reg    nan G_mae 0.024316 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:50,  1.52s/it] \n",
      "Epoch 152 D_loss 0.030162 D_reg 10350.150879 G_loss -0.838150 G_reg    nan G_mae 0.025622 p_real 0.0040 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:51,  1.53s/it]\n",
      "Epoch 153 D_loss 0.034778 D_reg 9379.633057 G_loss -0.846146 G_reg    nan G_mae 0.025008 p_real 0.0038 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 154 D_loss 0.035598 D_reg 18352.401042 G_loss -0.844644 G_reg    nan G_mae 0.026349 p_real 0.0039 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:43<01:56,  1.60s/it]\n",
      "Epoch 155 D_loss 0.037004 D_reg 28088.772135 G_loss -0.840617 G_reg    nan G_mae 0.025396 p_real 0.0039 real_acc 0.8958 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:52,  1.54s/it]\n",
      "Epoch 156 D_loss 0.034618 D_reg 59862.587891 G_loss -0.839051 G_reg    nan G_mae 0.027368 p_real 0.0040 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:48,  1.48s/it]\n",
      "Epoch 157 D_loss 0.042183 D_reg 32149.911458 G_loss -0.844321 G_reg    nan G_mae 0.024543 p_real 0.0039 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:38<01:44,  1.44s/it]\n",
      "Epoch 158 D_loss 0.031853 D_reg 11395.234375 G_loss -0.843877 G_reg    nan G_mae 0.025483 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:41<01:51,  1.53s/it]\n",
      "Epoch 159 D_loss 0.031646 D_reg 19002.228516 G_loss -0.841637 G_reg    nan G_mae 0.025565 p_real 0.0038 real_acc 0.9167 fake_acc 0.0208:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 160 D_loss 0.037310 D_reg 44235.766276 G_loss -0.842935 G_reg    nan G_mae 0.025058 p_real 0.0039 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:41<01:52,  1.54s/it]\n",
      "Epoch 161 D_loss 0.032297 D_reg 10442.569010 G_loss -0.838028 G_reg    nan G_mae 0.025435 p_real 0.0039 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:50,  1.51s/it] \n",
      "Epoch 162 D_loss 0.020510 D_reg 10934.163411 G_loss -0.844397 G_reg    nan G_mae 0.024220 p_real 0.0038 real_acc 0.9479 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:44,  1.44s/it]\n",
      "Epoch 163 D_loss 0.058938 D_reg 14111.032227 G_loss -0.847219 G_reg    nan G_mae 0.024124 p_real 0.0038 real_acc 0.9062 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:45,  1.44s/it]\n",
      "Epoch 164 D_loss 0.029915 D_reg 13869.385091 G_loss -0.842929 G_reg    nan G_mae 0.026661 p_real 0.0040 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:41<01:53,  1.55s/it]\n",
      "Epoch 165 D_loss 0.029557 D_reg 23178.002604 G_loss -0.846614 G_reg    nan G_mae 0.025118 p_real 0.0039 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 166 D_loss 0.031975 D_reg 15474.862305 G_loss -0.844151 G_reg    nan G_mae 0.026935 p_real 0.0039 real_acc 0.9167 fake_acc 0.0312:  27%|██▋       | 27/100 [00:40<01:48,  1.49s/it]\n",
      "Epoch 167 D_loss 0.017851 D_reg 15329.323405 G_loss -0.840373 G_reg    nan G_mae 0.024755 p_real 0.0038 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 168 D_loss 0.027823 D_reg 29594.478841 G_loss -0.839312 G_reg    nan G_mae 0.025496 p_real 0.0039 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it]\n",
      "Epoch 169 D_loss 0.046448 D_reg 16288.127604 G_loss -0.846613 G_reg    nan G_mae 0.025647 p_real 0.0040 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:46,  1.45s/it]\n",
      "Epoch 170 D_loss 0.044158 D_reg 13502.333984 G_loss -0.848116 G_reg    nan G_mae 0.024580 p_real 0.0037 real_acc 0.9062 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:50,  1.51s/it]\n",
      "Epoch 171 D_loss 0.036947 D_reg 7226.489095 G_loss -0.843813 G_reg    nan G_mae 0.025764 p_real 0.0039 real_acc 0.9062 fake_acc 0.0312:  27%|██▋       | 27/100 [00:40<01:48,  1.48s/it]\n",
      "Epoch 172 D_loss 0.010476 D_reg 13035.590495 G_loss -0.841056 G_reg    nan G_mae 0.025902 p_real 0.0039 real_acc 0.9792 fake_acc 0.0521:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 173 D_loss 0.037189 D_reg 12200.837891 G_loss -0.839109 G_reg    nan G_mae 0.024571 p_real 0.0039 real_acc 0.9271 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 174 D_loss 0.017077 D_reg 6927.904460 G_loss -0.840532 G_reg    nan G_mae 0.024940 p_real 0.0038 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:44,  1.44s/it]\n",
      "Epoch 175 D_loss 0.041702 D_reg 12038.660482 G_loss -0.844564 G_reg    nan G_mae 0.025556 p_real 0.0038 real_acc 0.9375 fake_acc 0.0417:  27%|██▋       | 27/100 [00:42<01:55,  1.58s/it]\n",
      "Epoch 176 D_loss 0.030271 D_reg 11329.801432 G_loss -0.846041 G_reg    nan G_mae 0.024195 p_real 0.0038 real_acc 0.9375 fake_acc 0.0625:  27%|██▋       | 27/100 [00:41<01:51,  1.53s/it]\n",
      "Epoch 177 D_loss 0.036128 D_reg 14552.007650 G_loss -0.840019 G_reg    nan G_mae 0.025704 p_real 0.0039 real_acc 0.9167 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 178 D_loss 0.018650 D_reg 21912.393392 G_loss -0.839642 G_reg    nan G_mae 0.024850 p_real 0.0039 real_acc 0.9479 fake_acc 0.0521:  27%|██▋       | 27/100 [00:37<01:41,  1.39s/it]\n",
      "Epoch 179 D_loss -0.003606 D_reg 18466.450195 G_loss -0.846717 G_reg    nan G_mae 0.024712 p_real 0.0038 real_acc 0.9688 fake_acc 0.0625:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it]\n",
      "Epoch 180 D_loss 0.027327 D_reg 6131.034912 G_loss -0.840933 G_reg    nan G_mae 0.024474 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:37<01:40,  1.37s/it]\n",
      "Epoch 181 D_loss 0.038699 D_reg 11667.887044 G_loss -0.840806 G_reg    nan G_mae 0.025449 p_real 0.0040 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:38<01:43,  1.41s/it]\n",
      "Epoch 182 D_loss 0.035201 D_reg 9810.714193 G_loss -0.838077 G_reg    nan G_mae 0.025355 p_real 0.0039 real_acc 0.8958 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it] \n",
      "Epoch 183 D_loss 0.040907 D_reg 13316.334310 G_loss -0.844129 G_reg    nan G_mae 0.025429 p_real 0.0039 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:37<01:42,  1.40s/it]\n",
      "Epoch 184 D_loss 0.012431 D_reg 22127.690430 G_loss -0.845849 G_reg    nan G_mae 0.024810 p_real 0.0039 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it] \n",
      "Epoch 185 D_loss 0.017932 D_reg 25457.490885 G_loss -0.849917 G_reg    nan G_mae 0.025335 p_real 0.0039 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:40<01:49,  1.50s/it] \n",
      "Epoch 186 D_loss 0.039789 D_reg 42994.887370 G_loss -0.842857 G_reg    nan G_mae 0.024891 p_real 0.0038 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:39<01:46,  1.46s/it]\n",
      "Epoch 187 D_loss 0.009614 D_reg 13290.305013 G_loss -0.845170 G_reg    nan G_mae 0.024492 p_real 0.0038 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:39<01:45,  1.45s/it] \n",
      "Epoch 188 D_loss 0.030777 D_reg 28828.416667 G_loss -0.839171 G_reg    nan G_mae 0.024173 p_real 0.0039 real_acc 0.9479 fake_acc 0.0417:  27%|██▋       | 27/100 [00:38<01:44,  1.43s/it]\n",
      "Epoch 189 D_loss 0.032520 D_reg 15535.340169 G_loss -0.840855 G_reg    nan G_mae 0.025095 p_real 0.0039 real_acc 0.9167 fake_acc 0.0312:  27%|██▋       | 27/100 [00:40<01:49,  1.49s/it]\n",
      "Epoch 190 D_loss 0.051552 D_reg 11251.522786 G_loss -0.845878 G_reg    nan G_mae 0.025986 p_real 0.0038 real_acc 0.9479 fake_acc 0.0312:  27%|██▋       | 27/100 [00:41<01:51,  1.53s/it]\n",
      "Epoch 191 D_loss 0.028612 D_reg 49926.049479 G_loss -0.839801 G_reg    nan G_mae 0.024393 p_real 0.0038 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:37<01:42,  1.41s/it]\n",
      "Epoch 192 D_loss 0.031638 D_reg 12537.913737 G_loss -0.840306 G_reg    nan G_mae 0.026112 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:37<01:40,  1.38s/it]\n",
      "Epoch 193 D_loss 0.023896 D_reg 9666.356445 G_loss -0.834736 G_reg    nan G_mae 0.025342 p_real 0.0039 real_acc 0.9479 fake_acc 0.0208:  27%|██▋       | 27/100 [00:40<01:48,  1.48s/it] \n",
      "Epoch 194 D_loss 0.038625 D_reg 12464.808919 G_loss -0.844361 G_reg    nan G_mae 0.024064 p_real 0.0038 real_acc 0.9375 fake_acc 0.0521:  27%|██▋       | 27/100 [00:39<01:47,  1.47s/it]\n",
      "Epoch 195 D_loss 0.042787 D_reg 8506.580566 G_loss -0.837264 G_reg    nan G_mae 0.025082 p_real 0.0040 real_acc 0.9271 fake_acc 0.0521:  27%|██▋       | 27/100 [00:37<01:42,  1.41s/it] \n",
      "Epoch 196 D_loss -0.007267 D_reg 9096.970052 G_loss -0.847699 G_reg    nan G_mae 0.025742 p_real 0.0038 real_acc 0.9583 fake_acc 0.0312:  27%|██▋       | 27/100 [00:38<01:44,  1.43s/it]\n",
      "Epoch 197 D_loss 0.043911 D_reg 11840.967773 G_loss -0.839869 G_reg    nan G_mae 0.025159 p_real 0.0039 real_acc 0.9167 fake_acc 0.0521:  27%|██▋       | 27/100 [00:38<01:43,  1.41s/it]\n",
      "Epoch 198 D_loss 0.022466 D_reg 15415.614258 G_loss -0.844994 G_reg    nan G_mae 0.024871 p_real 0.0039 real_acc 0.9375 fake_acc 0.0312:  27%|██▋       | 27/100 [00:37<01:41,  1.39s/it]\n",
      "Epoch 199 D_loss 0.048261 D_reg 18320.420247 G_loss -0.840101 G_reg    nan G_mae 0.024805 p_real 0.0039 real_acc 0.9167 fake_acc 0.0417:  27%|██▋       | 27/100 [00:40<01:48,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ### Train\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, x in dual_iter:\n",
    "        \n",
    "        img = x[\"input\"].squeeze(2)[:, :, :32, :32]\n",
    "        label = x[\"label\"].squeeze(2)[:, :, :32, :32]\n",
    "            \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(label.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(label.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        input_img = Variable(img.type(Tensor))\n",
    "        real_imgs = Variable(label.type(Tensor))\n",
    "                \n",
    "        # Sample noise as generator input\n",
    "        #z = Variable(Tensor(np.random.normal(0, 1.0, (holo_img.shape[0], 512))))\n",
    "        b, c, l, w = label.shape\n",
    "        z = Tensor(np.random.normal(0, 1.0, (b, 1, l, w)))\n",
    "        # C-GAN-like input using the synthethic image as conditional input\n",
    "        z = torch.cat([input_img, z], 1)\n",
    "        # Generate a batch of images\n",
    "        \n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Discriminate the fake images\n",
    "        _, verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "            \n",
    "        if (i + 1) % train_gen_every == 0:\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            \n",
    "            # measure the generator's ability to fool the discriminator\n",
    "            if epoch >= 10:\n",
    "                if adv_loss == 'wgan-gp':\n",
    "                    g_loss = -verdict.mean()\n",
    "                elif adv_loss == 'hinge':\n",
    "                    g_loss = -verdict.mean()\n",
    "                elif adv_loss == 'bce':\n",
    "                    g_loss = adversarial_loss(verdict, valid)\n",
    "                train_results[\"g_loss\"].append(g_loss.item())\n",
    "                \n",
    "                g_loss *= 0.01\n",
    "            \n",
    "            # compute MAE\n",
    "            if epoch >= 10:\n",
    "                g_mae = torch.nn.L1Loss()(gen_imgs, real_imgs.to(device))\n",
    "                train_results[\"g_mae\"].append(g_mae.item())\n",
    "                g_loss += g_mae\n",
    "            else:\n",
    "                g_loss = torch.nn.L1Loss()(gen_imgs, real_imgs.to(device))\n",
    "                train_results[\"g_mae\"].append(g_loss.item())\n",
    "                \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # compute perception scores\n",
    "            p_score_real = perceptual_alex(gen_imgs, real_imgs).mean()\n",
    "            train_results[\"p_real\"].append(p_score_real.item())\n",
    "            \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        if (i + 1) % train_disc_every == 0 and epoch >= 10:\n",
    "        \n",
    "            optimizer_D.zero_grad()\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            _, disc_real = discriminator(real_imgs)\n",
    "            _, disc_synth = discriminator(gen_imgs.detach())\n",
    "            \n",
    "            train_results[\"real_acc\"].append(((disc_real > threshold) == valid).float().mean().item())\n",
    "            train_results[\"fake_acc\"].append(((disc_synth > threshold) == fake).float().mean().item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                real_loss = -torch.mean(disc_real) \n",
    "                fake_loss = disc_synth.mean() \n",
    "            elif adv_loss == 'hinge':\n",
    "                real_loss = torch.nn.ReLU()(1.0 - disc_real).mean() \n",
    "                fake_loss = torch.nn.ReLU()(1.0 + disc_synth).mean()             \n",
    "            elif adv_loss == 'bce':\n",
    "                real_loss = adversarial_loss(disc_real, valid) \n",
    "                fake_loss = adversarial_loss(disc_synth, fake) \n",
    "                \n",
    "            d_loss = real_loss + fake_loss \n",
    "            train_results[\"d_loss\"].append(d_loss.item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "                interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "                out = discriminator(interpolated)[1]\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "                d_loss_reg = lambda_gp * d_loss_gp\n",
    "                d_loss += d_loss_reg\n",
    "                train_results[\"d_reg\"].append(d_loss_reg.item())\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print_str =  f'Epoch {epoch}'\n",
    "        print_str += f' D_loss {np.mean(train_results[\"d_loss\"]):.6f}'\n",
    "        if adv_loss == 'wgan-gp':\n",
    "            print_str += f' D_reg {np.mean(train_results[\"d_reg\"]):.6f}'\n",
    "        print_str += f' G_loss {np.mean(train_results[\"g_loss\"]):6f}'\n",
    "        print_str += f' G_reg {np.mean(train_results[\"g_reg\"]):6f}'\n",
    "        print_str += f' G_mae {np.mean(train_results[\"g_mae\"]):6f}'\n",
    "        print_str += f' p_real {np.mean(train_results[\"p_real\"]):.4f}'\n",
    "        print_str += f' real_acc {np.mean(train_results[\"real_acc\"]):.4f}'\n",
    "        print_str += f' fake_acc {np.mean(train_results[\"fake_acc\"]):.4f}'\n",
    "        dual_iter.set_description(print_str)\n",
    "        dual_iter.refresh()\n",
    "        \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "        \n",
    "    # Save the dataframe to disk\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"d_loss\"].append(np.mean(train_results[\"d_loss\"]))\n",
    "    if adv_loss == 'wgan-gp':\n",
    "        results[\"d_loss_reg\"].append(np.mean(train_results[\"d_reg\"]))\n",
    "    results[\"g_loss\"].append(np.mean(train_results[\"g_loss\"]))\n",
    "    results[\"g_reg\"].append(np.mean(train_results[\"g_reg\"]))\n",
    "    results[\"perception\"].append(np.mean(train_results[\"p_real\"]))\n",
    "    results[\"real_acc\"].append(np.mean(train_results[\"real_acc\"]))\n",
    "    results[\"fake_acc\"].append(np.mean(train_results[\"syn_acc\"]))\n",
    "\n",
    "    metric = \"custom\"\n",
    "    metric_value = results[\"perception\"][-1]\n",
    "    results[metric].append(metric_value)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    save_image(real_imgs.data[:9], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=3, normalize=True)\n",
    "    save_image(gen_imgs.data[:9], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=3, normalize=True)\n",
    "\n",
    "    # Save the model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')\n",
    "    \n",
    "    # Anneal learning rates \n",
    "    lr_G_decay.step(epoch)\n",
    "    lr_D_decay.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0203, device='cuda:0', grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
