{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46ec794-7190-4127-9c45-19bad6218d33",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d746a1b0-1265-48d9-a1fb-cd9621e6ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import som_analysis\n",
    "import cluster_analysis\n",
    "import narm_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0046f-94e8-4e58-bc39-62bf5fa2c426",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e98c69-af7c-4829-bca6-988812581d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_indx(ds, mo_init=9, mo_end=2):\n",
    "    \"\"\"\n",
    "    Extract indices for cold season.\n",
    "    Grabbing Sept thru February init, for Oct thru March predictions.\n",
    "    \"\"\"\n",
    "    dt_array = pd.to_datetime(ds['time'])\n",
    "    return xr.where((dt_array.month >= mo_init) | (dt_array.month <= mo_end),\n",
    "                    True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eccf52-3d6d-470a-b583-1a941401e5da",
   "metadata": {},
   "source": [
    "## open and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd90e72-c9cf-4fd3-92aa-f941b8b7f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region for clustering\n",
    "lat0 = 10\n",
    "lat1 = 70\n",
    "lon0 = -150\n",
    "lon1 = -40\n",
    "\n",
    "# open era5 data and slice\n",
    "ds_era5 = narm_analysis.era5_z500(lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1)\n",
    "\n",
    "# era5 anomalies\n",
    "ds_era5_anom = narm_analysis.era5_climo_wrs(\n",
    "    ds_era5, rolling_days=5, variable='clim')\n",
    "\n",
    "# restructure era5 array for machine learning training (SONDJFM)\n",
    "ds_era5_anom = ds_era5_anom[get_cold_indx(\n",
    "    ds_era5_anom, mo_init=10, mo_end=3), ...]\n",
    "ds_era5_train = ds_era5_anom.stack(\n",
    "    flat=('lat', 'lon')).transpose('time', 'flat').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a09f-1ef8-4356-94d5-06998421187c",
   "metadata": {},
   "source": [
    "## pca and kmeans with era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b625fee7-0663-4615-abd6-d455765106cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: [25.95315607 17.65410568 11.94871708  9.0784389   7.98100848  6.14181738\n",
      "  4.32605934  2.61658689  2.22642929  2.17049559  1.49813958  1.22541708]\n",
      "Cumulative sum of variance explained for EOF1 and EOF2: [25.95315607 43.60726175 55.55597883 64.63441774 72.61542622 78.7572436\n",
      " 83.08330294 85.69988983 87.92631912 90.09681471 91.59495429 92.82037136]\n",
      "inertia: 39379.205423124506\n"
     ]
    }
   ],
   "source": [
    "# create pca object\n",
    "pca_obj = PCA(12, whiten=True)\n",
    "\n",
    "# fit pca with era5\n",
    "pca_obj = pca_obj.fit(ds_era5_train)\n",
    "\n",
    "# transform era5 data with pca\n",
    "ds_era5_train = pca_obj.transform(ds_era5_train)\n",
    "\n",
    "print(f'Variance explained: {pca_obj.explained_variance_ratio_ * 100}')\n",
    "print(\n",
    "f'Cumulative sum of variance explained for EOF1 and EOF2: {np.cumsum(pca_obj.explained_variance_ratio_) * 100}'\n",
    ")\n",
    "\n",
    "# train kmeans\n",
    "k_means = KMeans(n_clusters=4,\n",
    "                 init='k-means++',\n",
    "                 n_init=10000,\n",
    "                 max_iter=300,\n",
    "                 tol=0.0001,\n",
    "                 verbose=0,\n",
    "                 random_state=0).fit(ds_era5_train)\n",
    "\n",
    "print(f'inertia: {k_means.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51731e-42c4-444c-9199-d9678b76d8a4",
   "metadata": {},
   "source": [
    "## load data with lead time bias corrected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ed2b29-43ea-4c0b-b6b0-dab1f3550c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "z500_era5, z500_era5_dt = som_analysis.open_era5_files(\n",
    "    variable='z500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "z500_cesm, z500_cesm_dt = som_analysis.open_cesm_files(\n",
    "    variable='zg_500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# restructure arrays\n",
    "z500_standard_era5 = z500_era5.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')\n",
    "z500_standard_cesm = z500_cesm.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67582b5-a219-4d7d-a4d4-c82f4217a777",
   "metadata": {},
   "source": [
    "## composites of the weather types/regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0ed533-9c49-442c-b237-1d4d4069a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab cluster indices\n",
    "\n",
    "z500_era5_tmp_1, z500_era5_tmp_2, z500_era5_tmp_3, z500_era5_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_era5, k_means, pca_obj, use_pca=True)\n",
    "\n",
    "z500_cesm_tmp_1, z500_cesm_tmp_2, z500_cesm_tmp_3, z500_cesm_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_cesm, k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641ea18-c965-4790-9aca-573694efd732",
   "metadata": {},
   "source": [
    "## outgoing longwave radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf1fe9c-23e4-42e2-b122-3a5409a7cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "rlut_era5, _ = som_analysis.open_era5_files(\n",
    "    variable='ttr', return_time=True,\n",
    "    lat0=-50, lat1=50, lon0=20-360, lon1=-20,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "rlut_cesm, _ = som_analysis.open_cesm_files(\n",
    "    variable='rlut', return_time=True,\n",
    "    lat0=-50, lat1=50, lon0=20-360, lon1=-20,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# restructure data array\n",
    "rlut_era5_tmp = rlut_era5.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')\n",
    "\n",
    "rlut_cesm_tmp = rlut_cesm.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1282863c-1027-40a5-a189-1966dbd41515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters using indices\n",
    "\n",
    "rlut_era5_tmp_01 = rlut_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "rlut_era5_tmp_02 = rlut_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "rlut_era5_tmp_03 = rlut_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "rlut_era5_tmp_04 = rlut_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "rlut_cesm_tmp_01 = rlut_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_1, :, :]\n",
    "rlut_cesm_tmp_02 = rlut_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_2, :, :]\n",
    "rlut_cesm_tmp_03 = rlut_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_3, :, :]\n",
    "rlut_cesm_tmp_04 = rlut_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_4, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb7f6a-b374-41bb-9c87-ee5455f3fe59",
   "metadata": {},
   "source": [
    "## sea surface temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc30ab5-3b6d-47a0-a642-95104c15f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "sstk_era5, _ = som_analysis.open_era5_files(\n",
    "    variable='sstk', return_time=True,\n",
    "    lat0=-30, lat1=70, lon0=20-360, lon1=-20,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "sstk_cesm, _ = som_analysis.open_cesm_files(\n",
    "    variable='sst', return_time=True,\n",
    "    lat0=-30, lat1=70, lon0=20-360, lon1=-20,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# restructure data array\n",
    "sstk_era5_tmp = sstk_era5.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')\n",
    "\n",
    "sstk_cesm_tmp = sstk_cesm.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee050b6-f845-4495-9185-33214bd9deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters using indices\n",
    "\n",
    "sstk_era5_tmp_01 = sstk_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "sstk_era5_tmp_02 = sstk_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "sstk_era5_tmp_03 = sstk_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "sstk_era5_tmp_04 = sstk_era5_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "sstk_cesm_tmp_01 = sstk_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_1, :, :]\n",
    "sstk_cesm_tmp_02 = sstk_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_2, :, :]\n",
    "sstk_cesm_tmp_03 = sstk_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_3, :, :]\n",
    "sstk_cesm_tmp_04 = sstk_cesm_tmp.unstack('flat').transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_4, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95150d1-6852-4057-b034-b5cf09726641",
   "metadata": {},
   "source": [
    "## bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58f9fac-970d-4b1a-9522-cbe2c830918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday = 1\n",
    "seconday = 14\n",
    "\n",
    "lons_rlut = rlut_era5_tmp_01.lon.values\n",
    "lats_rlut = rlut_era5_tmp_01.lat.values\n",
    "\n",
    "lons_sstk = sstk_era5_tmp_01.lon.values\n",
    "lats_sstk = sstk_era5_tmp_01.lat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d769b5-477a-4526-ac2b-f75e294e161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_num_init_ = 1000\n",
    "boot_num_iter_ = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa236d-72f4-45d4-9a53-e5ea4af0fdd9",
   "metadata": {},
   "source": [
    "## olr era5 bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95251e8d-5afa-4c06-bba0-8c470c60733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = rlut_era5_tmp.unstack('new').unstack('flat').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c6e1cd-9a49-44a6-ac8f-6f59eb69c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_era5_tmp_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_era5_wr1_week12/olr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7a7e77-ecec-4987-a2a9-839a3d0fe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_era5_tmp_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(4620, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_era5_wr2_week12/olr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1301b57-d617-4226-9991-4db1e36bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_era5_tmp_03.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(7937, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_era5_wr3_week12/olr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296a50a-44be-4b93-bccf-0fe4f2e4d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_era5_tmp_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(7840, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_era5_wr4_week12/olr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f2fab-ecbf-4635-bc4f-fcd15b7beb4e",
   "metadata": {},
   "source": [
    "## olr cesm bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5555d2d5-fe67-4f33-9201-5f81d0ea970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = rlut_cesm_tmp.unstack('new').unstack('flat').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5edabe-1246-4fd3-9c2f-8d17ca6a919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_cesm_tmp_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr1_week12/olr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23724214-61ca-404d-b716-8c258458b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_cesm_tmp_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(3884, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr2_week12/olr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa0fb-9180-45ff-bba9-6b582af7a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_cesm_tmp_03.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(6619, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr3_week12/olr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423ddea5-1659-4068-9154-c2cf04417fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = rlut_cesm_tmp_04.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(8957, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_rlut),\n",
    "            lat=([\"lat\"], lats_rlut),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr4_week12/olr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e81261-8627-4980-b977-f6e34f93917c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## sst era5 bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978ed81c-ea5c-47a0-b650-3e33ebfcfe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = sstk_era5_tmp.unstack('new').unstack('flat').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae4e4d-3550-4618-9302-1a72c34c6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_193149/971673349.py:10: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = sstk_era5_tmp_01.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_era5_wr1_week12/sst_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a897305-388b-4006-8f91-bdf9045dc724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_164081/2357127251.py:10: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = sstk_era5_tmp_02.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(2946, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_era5_wr2_week12/sst_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04561d2f-ad24-48e0-b84b-ac6ed6f14e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_184509/2815977584.py:10: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = sstk_era5_tmp_03.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(5547, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_era5_wr3_week12/sst_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9221a42-9c73-4f70-a068-af680b202164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_121379/513863038.py:10: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = sstk_era5_tmp_04.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(8064, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_era5_wr4_week12/sst_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2430d50-a9a9-4e0e-847e-d96640465d30",
   "metadata": {},
   "source": [
    "## sst cesm bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2f9c2c-be4b-488f-a9cb-c22d4970e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = sstk_cesm_tmp.unstack('new').unstack('flat').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5b013b-7971-425a-89e1-85c5051fac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = sstk_cesm_tmp_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr1_week12/sst_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264b0a58-b3aa-472f-b24b-02c77b8ccd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = sstk_cesm_tmp_02.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(3844, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr2_week12/sst_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df01d0f-85c6-4e2e-a400-03c3a465eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = sstk_cesm_tmp_03.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(7662, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr3_week12/sst_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77826e5c-5977-4684-bc98-bd815a28da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = sstk_cesm_tmp_04.unstack('new').isel(\n",
    "    lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(\n",
    "        tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons_sstk),\n",
    "            lat=([\"lat\"], lats_sstk),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr4_week12/sst_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89827-7ff0-47eb-89d2-df1a11b5ccc4",
   "metadata": {},
   "source": [
    "## compute bootstrap percentiles (olr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b453122-6df9-4735-a259-23fd3d0f19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_1 = 0.025\n",
    "lev_2 = 0.975\n",
    "lev_3 = 0.005\n",
    "lev_4 = 0.995\n",
    "\n",
    "# era5\n",
    "\n",
    "tmp_era5_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_era5_wr1_week12/olr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_era5_wr2_week12/olr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_era5_wr3_week12/olr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_era5_wr4_week12/olr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# cesm\n",
    "\n",
    "tmp_cesm_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr1_week12/olr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr2_week12/olr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr3_week12/olr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/olr_cesm_wr4_week12/olr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8d6e5-82e8-4a1a-afc5-f6c6b42f04e3",
   "metadata": {},
   "source": [
    "## assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f61bcd6-7db7-43f7-b8bf-06220601b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_olr = xr.Dataset(\n",
    "\n",
    "    data_vars=dict(\n",
    "\n",
    "        wr1_era5=([\"lat\", \"lon\"], rlut_era5_tmp_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr2_era5=([\"lat\", \"lon\"], rlut_era5_tmp_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr3_era5=([\"lat\", \"lon\"], rlut_era5_tmp_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr4_era5=([\"lat\", \"lon\"], rlut_era5_tmp_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "\n",
    "        wr1_era5_025=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_975=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_005=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_995=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_era5_025=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_975=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_005=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_995=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_era5_025=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_975=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_005=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_995=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_era5_025=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_975=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_005=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_995=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr1_cesm=([\"lat\", \"lon\"], rlut_cesm_tmp_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr2_cesm=([\"lat\", \"lon\"], rlut_cesm_tmp_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr3_cesm=([\"lat\", \"lon\"], rlut_cesm_tmp_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "        wr4_cesm=([\"lat\", \"lon\"], rlut_cesm_tmp_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).values),\n",
    "\n",
    "        wr1_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "    ),\n",
    "\n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], lons_rlut),\n",
    "        lat=([\"lat\"], lats_rlut),\n",
    "    ),\n",
    "\n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42f3e4-1701-4079-b546-b8481305e712",
   "metadata": {},
   "source": [
    "## save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60fc6170-f825-4550-bc09-599a5d851c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_olr.to_netcdf(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/rlut_week12_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a3fb3-65aa-4e46-8509-4b7b8653cf63",
   "metadata": {},
   "source": [
    "## compute bootstrap percentiles (sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "825708d5-8103-484f-b6fa-8a53f6e4c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_1 = 0.025\n",
    "lev_2 = 0.975\n",
    "lev_3 = 0.005\n",
    "lev_4 = 0.995\n",
    "\n",
    "# era5\n",
    "\n",
    "tmp_era5_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_era5_wr1_week12/sst_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_era5_wr2_week12/sst_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_era5_wr3_week12/sst_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_era5_wr4_week12/sst_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# cesm\n",
    "\n",
    "tmp_cesm_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr1_week12/sst_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr2_week12/sst_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr3_week12/sst_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sst_cesm_wr4_week12/sst_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157be32e-09de-459f-a553-b58767467ed5",
   "metadata": {},
   "source": [
    "## assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12e2bab-7f82-46eb-ab5a-2b3c4dd9a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    }
   ],
   "source": [
    "ds_sst = xr.Dataset(\n",
    "    \n",
    "    data_vars=dict(\n",
    "        \n",
    "        wr1_era5=([\"lat\", \"lon\"], sstk_era5_tmp_01.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr2_era5=([\"lat\", \"lon\"], sstk_era5_tmp_02.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr3_era5=([\"lat\", \"lon\"], sstk_era5_tmp_03.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr4_era5=([\"lat\", \"lon\"], sstk_era5_tmp_04.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        \n",
    "        wr1_era5_025=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_975=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_005=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_995=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr2_era5_025=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_975=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_005=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_995=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr3_era5_025=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_975=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_005=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_995=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr4_era5_025=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_975=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_005=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_995=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        \n",
    "        wr1_cesm=([\"lat\", \"lon\"], sstk_cesm_tmp_01.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr2_cesm=([\"lat\", \"lon\"], sstk_cesm_tmp_02.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr3_cesm=([\"lat\", \"lon\"], sstk_cesm_tmp_03.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        wr4_cesm=([\"lat\", \"lon\"], sstk_cesm_tmp_04.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "                                      new=('lead','time')).mean('new',skipna=True).values),\n",
    "        \n",
    "        wr1_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr2_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr3_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr4_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "    ),\n",
    "    \n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], lons_sstk),\n",
    "        lat=([\"lat\"], lats_sstk),\n",
    "    ),\n",
    "    \n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba24c-0479-4b6a-9899-c766cdbaabbe",
   "metadata": {},
   "source": [
    "## save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a97d2725-3660-4ebd-88b7-1a116ba2bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.to_netcdf(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/sstk_week12_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcaef35-26b9-42ab-99e3-db242ef5a52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87becea-b1c2-46ba-b6c9-9ef53e983f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-myenv-tfgpu]",
   "language": "python",
   "name": "conda-env-miniconda3-myenv-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
