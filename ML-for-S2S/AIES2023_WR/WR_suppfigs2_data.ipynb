{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46ec794-7190-4127-9c45-19bad6218d33",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d746a1b0-1265-48d9-a1fb-cd9621e6ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cf\n",
    "import shapely.geometry as sgeom\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import xskillscore as xs\n",
    "\n",
    "import som_analysis\n",
    "import cluster_analysis\n",
    "import narm_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0046f-94e8-4e58-bc39-62bf5fa2c426",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c339c36-c2f3-48f3-b4e5-998914deec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_insetmap(axes_extent, map_extent, lons, lats, temp_data, \n",
    "                 vmin, vmax, cmap='coolwarm'):\n",
    "\n",
    "    use_projection = ccrs.Mercator()     # preserve shape well\n",
    "    geodetic = ccrs.Geodetic(globe=ccrs.Globe(datum='WGS84'))\n",
    "\n",
    "    sub_ax = plt.axes(axes_extent, projection=use_projection)  # normal units\n",
    "\n",
    "    sub_ax.set_extent(map_extent, geodetic)  # map extents\n",
    "    sub_ax.coastlines(linewidth=0.35, zorder=10)\n",
    "\n",
    "    sub_ax.pcolormesh(lons, lats, temp_data, transform=ccrs.PlateCarree(),\n",
    "                      vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "    extent_box = sgeom.box(\n",
    "        map_extent[0], map_extent[2], map_extent[1], map_extent[3])\n",
    "    sub_ax.add_geometries(\n",
    "        [extent_box], ccrs.PlateCarree(), color='none', linewidth=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e98c69-af7c-4829-bca6-988812581d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_indx(ds, mo_init=9, mo_end=2):\n",
    "    \"\"\"\n",
    "    Extract indices for cold season.\n",
    "    Grabbing Sept thru February init, for Oct thru March predictions.\n",
    "    \"\"\"\n",
    "    dt_array = pd.to_datetime(ds['time'])\n",
    "    return xr.where((dt_array.month >= mo_init) | (\n",
    "        dt_array.month <= mo_end), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eccf52-3d6d-470a-b583-1a941401e5da",
   "metadata": {},
   "source": [
    "## open and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd90e72-c9cf-4fd3-92aa-f941b8b7f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region for clustering\n",
    "lat0 = 10\n",
    "lat1 = 70\n",
    "lon0 = -150\n",
    "lon1 = -40\n",
    "\n",
    "# open era5 data and slice\n",
    "ds_era5 = narm_analysis.era5_z500(lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1)\n",
    "\n",
    "# era5 anomalies\n",
    "ds_era5_anom = narm_analysis.era5_climo_wrs(\n",
    "    ds_era5, rolling_days=5, variable='clim')\n",
    "\n",
    "# restructure era5 array for machine learning training (SONDJFM)\n",
    "ds_era5_anom = ds_era5_anom[get_cold_indx(\n",
    "    ds_era5_anom, mo_init=10, mo_end=3), ...]\n",
    "ds_era5_train = ds_era5_anom.stack(\n",
    "    flat=('lat', 'lon')).transpose('time', 'flat').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a09f-1ef8-4356-94d5-06998421187c",
   "metadata": {},
   "source": [
    "## pca and kmeans with era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b625fee7-0663-4615-abd6-d455765106cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: [25.95315607 17.65410568 11.94871708  9.0784389   7.98100848  6.14181738\n",
      "  4.32605934  2.61658689  2.22642929  2.17049559  1.49813958  1.22541708]\n",
      "Cumulative sum of variance explained for EOF1 and EOF2: [25.95315607 43.60726175 55.55597883 64.63441774 72.61542622 78.7572436\n",
      " 83.08330294 85.69988983 87.92631912 90.09681471 91.59495429 92.82037136]\n",
      "inertia: 39379.205368509545\n"
     ]
    }
   ],
   "source": [
    "# create pca object\n",
    "pca_obj = PCA(12, whiten=True)\n",
    "\n",
    "# fit pca with era5\n",
    "pca_obj = pca_obj.fit(ds_era5_train)\n",
    "\n",
    "# transform era5 data with pca\n",
    "ds_era5_train = pca_obj.transform(ds_era5_train)\n",
    "\n",
    "print(f'Variance explained: {pca_obj.explained_variance_ratio_ * 100}')\n",
    "print(\n",
    "f'Cumulative sum of variance explained for EOF1 and EOF2: {np.cumsum(pca_obj.explained_variance_ratio_) * 100}'\n",
    ")\n",
    "\n",
    "# train kmeans\n",
    "k_means = KMeans(n_clusters=4,\n",
    "                 init='k-means++',\n",
    "                 n_init=10000,\n",
    "                 max_iter=300,\n",
    "                 tol=0.0001,\n",
    "                 verbose=0,\n",
    "                 random_state=0).fit(ds_era5_train)\n",
    "\n",
    "print(f'inertia: {k_means.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51731e-42c4-444c-9199-d9678b76d8a4",
   "metadata": {},
   "source": [
    "## load data with lead time bias corrected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ed2b29-43ea-4c0b-b6b0-dab1f3550c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "z500_era5, z500_era5_dt = som_analysis.open_era5_files(\n",
    "    variable='z500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "z500_cesm, z500_cesm_dt = som_analysis.open_cesm_files(\n",
    "    variable='zg_500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5,)\n",
    "\n",
    "# restructure arrays\n",
    "z500_standard_era5 = z500_era5.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')\n",
    "z500_standard_cesm = z500_cesm.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67582b5-a219-4d7d-a4d4-c82f4217a777",
   "metadata": {},
   "source": [
    "## composites of the weather types/regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0ed533-9c49-442c-b237-1d4d4069a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab cluster indices\n",
    "\n",
    "z500_era5_tmp_1, z500_era5_tmp_2, z500_era5_tmp_3, z500_era5_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_era5, k_means, pca_obj, use_pca=True)\n",
    "\n",
    "z500_cesm_tmp_1, z500_cesm_tmp_2, z500_cesm_tmp_3, z500_cesm_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_cesm, k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990b7-f326-4657-9895-163b60af3510",
   "metadata": {
    "tags": []
   },
   "source": [
    "## precipitation anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa163bf-cea7-4793-a3b6-72b0828a9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0_tmp = 10\n",
    "lat1_tmp = 75\n",
    "lon0_tmp = -165\n",
    "lon1_tmp = -40\n",
    "\n",
    "# precip\n",
    "\n",
    "# noaa data\n",
    "pr_noaa, _ = som_analysis.open_noaa_files(\n",
    "    variable='precip', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "mask = xr.where(~np.isnan(pr_noaa.isel(time=0, lead=0)), 1.0, np.nan)\n",
    "\n",
    "# era5 data\n",
    "pr_era5, _ = som_analysis.open_era5_files(\n",
    "    variable='tp', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "pr_era5 = pr_era5.where(mask == 1.0)\n",
    "\n",
    "# cesm data\n",
    "pr_cesm, _ = som_analysis.open_cesm_files(\n",
    "    variable='pr_sfc', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "pr_cesm = pr_cesm.where(mask == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcab496-9ae3-4d17-9c49-a43a2e53a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_noaa_01 = pr_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "pr_noaa_02 = pr_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "pr_noaa_03 = pr_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "pr_noaa_04 = pr_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "pr_era5_01 = pr_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "pr_era5_02 = pr_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "pr_era5_03 = pr_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "pr_era5_04 = pr_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "pr_cesm_01 = pr_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_1, :, :]\n",
    "pr_cesm_02 = pr_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_2, :, :]\n",
    "pr_cesm_03 = pr_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_3, :, :]\n",
    "pr_cesm_04 = pr_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_4, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef7650-bcf9-4e04-a8b7-7ea7933bc455",
   "metadata": {},
   "source": [
    "## bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dafafe-4b55-4210-ab61-f403950d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday = 1\n",
    "seconday = 14\n",
    "\n",
    "lons = pr_noaa_01.lon.values\n",
    "lats = pr_noaa_01.lat.values\n",
    "\n",
    "boot_num_init_ = 1000\n",
    "boot_num_iter_ = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169ad71-8b78-4f40-91bc-dc99209ad2b0",
   "metadata": {},
   "source": [
    "## noaa bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b5e72f-634a-4245-b1fa-ae9c548feae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = pr_noaa.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e66234-8844-4341-9bd3-7f8daf8f8a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_75766/1445797719.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_noaa_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(9205, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr1_week12/pr_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e0779c5-adf9-49f4-a31f-6c5feda55bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_75766/2475906466.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_noaa_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr2_week12/pr_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7dcf67-6ff5-41fb-8c5b-1016fc1adc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_75766/2217405587.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_noaa_03.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr3_week12/pr_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a429947e-fc24-4140-a502-3b66da889fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_75766/1143828362.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_noaa_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr4_week12/pr_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5164104-59f7-4bab-91ed-42478f2453e6",
   "metadata": {},
   "source": [
    "## era5 bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8934290-748c-4250-abfc-7e42325a9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = pr_era5.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6314b1a-d7ec-4f3d-83a0-bf3bb111af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_75766/1141947540.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_era5_01.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_era5_wr1_week12/pr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fa0f8a-79ca-4c9c-9b9e-1c31d17bce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_11541/2666484880.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_era5_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(7810, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_era5_wr2_week12/pr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d523b-f52b-4451-943d-dc06def53b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_11541/3624330448.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_era5_03.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_era5_wr3_week12/pr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d855c-e362-4400-bdbd-603220f07486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = pr_era5_04.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_era5_wr4_week12/pr_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e281b-3b2d-43b1-b133-b3d92b503517",
   "metadata": {},
   "source": [
    "## cesm bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95dcfa1-6c43-474c-a3b4-a8dfe911a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = pr_cesm.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c97e1-07c5-4636-9b0c-49876104aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = pr_cesm_01.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr1_week12/pr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67cb2a-c03b-4ed7-aff6-0a0678901b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = pr_cesm_02.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr2_week12/pr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74eb1e65-2dd1-4975-97f9-9e4e4646467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_109086/1752697186.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_cesm_03.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(4990, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr3_week12/pr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42012ed1-7fae-4059-9d51-3f7487beb4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_109086/3411249489.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = pr_cesm_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr4_week12/pr_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c98825-7466-4671-a6ee-bb7812e26547",
   "metadata": {},
   "source": [
    "## compute percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a300ab-0109-45c4-a9ac-298e92837710",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_1 = 0.025\n",
    "lev_2 = 0.975\n",
    "lev_3 = 0.005\n",
    "lev_4 = 0.995\n",
    "\n",
    "# ncpc\n",
    "\n",
    "tmp_ncpc_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr1_week12/pr_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr2_week12/pr_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr3_week12/pr_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_ncpc_wr4_week12/pr_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# era5\n",
    "\n",
    "tmp_era5_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_era5_wr1_week12/pr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_era5_wr2_week12/pr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_era5_wr3_week12/pr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_era5_wr4_week12/pr_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# cesm\n",
    "\n",
    "tmp_cesm_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr1_week12/pr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr2_week12/pr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr3_week12/pr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/pr_cesm_wr4_week12/pr_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc9c4d-4e1a-4b2d-825c-5bd2aec8ebc4",
   "metadata": {},
   "source": [
    "## assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e3b517e-0672-484e-b0d8-ad21a0ec3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    }
   ],
   "source": [
    "ds_pr = xr.Dataset(\n",
    "\n",
    "    data_vars=dict(\n",
    "\n",
    "        wr1_ncpc=([\"lat\", \"lon\"], pr_noaa_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_ncpc=([\"lat\", \"lon\"], pr_noaa_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_ncpc=([\"lat\", \"lon\"], pr_noaa_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_ncpc=([\"lat\", \"lon\"], pr_noaa_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr1_era5=([\"lat\", \"lon\"], pr_era5_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_era5=([\"lat\", \"lon\"], pr_era5_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_era5=([\"lat\", \"lon\"], pr_era5_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_era5=([\"lat\", \"lon\"], pr_era5_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_era5_025=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_975=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_005=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_995=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_era5_025=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_975=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_005=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_995=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_era5_025=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_975=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_005=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_995=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_era5_025=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_975=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_005=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_995=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr1_cesm=([\"lat\", \"lon\"], pr_cesm_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_cesm=([\"lat\", \"lon\"], pr_cesm_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_cesm=([\"lat\", \"lon\"], pr_cesm_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_cesm=([\"lat\", \"lon\"], pr_cesm_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "    ),\n",
    "\n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], lons),\n",
    "        lat=([\"lat\"], lats),\n",
    "    ),\n",
    "\n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c80b8-2c64-4738-8bd9-2718bdcf6279",
   "metadata": {},
   "source": [
    "## save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ebc730-69a0-4102-aaa2-26f84787c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pr.to_netcdf('/glade/scratch/molina/s2s/bootstrap/pr_week12_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996d979-0dee-4536-99e6-d8c4c1455b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6791b1-f79b-4d7a-8abf-c07358ed68c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-myenv-tfgpu]",
   "language": "python",
   "name": "conda-env-miniconda3-myenv-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
