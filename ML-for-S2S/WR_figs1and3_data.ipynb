{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46ec794-7190-4127-9c45-19bad6218d33",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d746a1b0-1265-48d9-a1fb-cd9621e6ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cf\n",
    "import shapely.geometry as sgeom\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import xskillscore as xs\n",
    "\n",
    "import som_analysis\n",
    "import cluster_analysis\n",
    "import narm_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0046f-94e8-4e58-bc39-62bf5fa2c426",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c339c36-c2f3-48f3-b4e5-998914deec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_insetmap(axes_extent, map_extent, lons, lats, temp_data, \n",
    "                 vmin, vmax, cmap='coolwarm'):\n",
    "    \n",
    "    use_projection = ccrs.Mercator()     # preserve shape well\n",
    "    geodetic = ccrs.Geodetic(globe=ccrs.Globe(datum='WGS84'))\n",
    "    \n",
    "    sub_ax = plt.axes(axes_extent, projection=use_projection)  # normal units\n",
    "    \n",
    "    sub_ax.set_extent(map_extent, geodetic)  # map extents\n",
    "    sub_ax.coastlines(linewidth=0.35, zorder=10)\n",
    "    \n",
    "    sub_ax.pcolormesh(lons, lats, temp_data, transform=ccrs.PlateCarree(),\n",
    "                      vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "    extent_box = sgeom.box(map_extent[0], map_extent[2], map_extent[1], map_extent[3])\n",
    "    sub_ax.add_geometries([extent_box], ccrs.PlateCarree(), color='none', linewidth=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e98c69-af7c-4829-bca6-988812581d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_indx(ds, mo_init=9, mo_end=2):\n",
    "    \"\"\"\n",
    "    Extract indices for cold season.\n",
    "    Grabbing Sept thru February init, for Oct thru March predictions.\n",
    "    \"\"\"\n",
    "    dt_array = pd.to_datetime(ds['time'])\n",
    "    return xr.where((dt_array.month>=mo_init) | (dt_array.month<=mo_end), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eccf52-3d6d-470a-b583-1a941401e5da",
   "metadata": {},
   "source": [
    "## open and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd90e72-c9cf-4fd3-92aa-f941b8b7f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region for clustering\n",
    "lat0=10; lat1=70; lon0=-150; lon1=-40\n",
    "\n",
    "# open era5 data and slice\n",
    "ds_era5 = narm_analysis.era5_z500(lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1)\n",
    "\n",
    "# era5 anomalies\n",
    "ds_era5_anom = narm_analysis.era5_climo_wrs(ds_era5, rolling_days=5, variable='clim')\n",
    "\n",
    "# restructure era5 array for machine learning training (SONDJFM)\n",
    "ds_era5_anom = ds_era5_anom[get_cold_indx(ds_era5_anom, mo_init=10, mo_end=3),...]\n",
    "ds_era5_train = ds_era5_anom.stack(flat=('lat','lon')).transpose('time','flat').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a09f-1ef8-4356-94d5-06998421187c",
   "metadata": {},
   "source": [
    "## pca and kmeans with era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b625fee7-0663-4615-abd6-d455765106cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: [25.95315607 17.65410568 11.94871708  9.0784389   7.98100848  6.14181738\n",
      "  4.32605934  2.61658689  2.22642929  2.17049559  1.49813958  1.22541708]\n",
      "Cumulative sum of variance explained for EOF1 and EOF2: [25.95315607 43.60726175 55.55597883 64.63441774 72.61542622 78.7572436\n",
      " 83.08330294 85.69988983 87.92631912 90.09681471 91.59495429 92.82037136]\n",
      "inertia: 39379.20536675407\n"
     ]
    }
   ],
   "source": [
    "# create pca object\n",
    "pca_obj = PCA(12, whiten=True)\n",
    "\n",
    "# fit pca with era5\n",
    "pca_obj = pca_obj.fit(ds_era5_train)\n",
    "\n",
    "# transform era5 data with pca\n",
    "ds_era5_train = pca_obj.transform(ds_era5_train)\n",
    "\n",
    "print(f'Variance explained: {pca_obj.explained_variance_ratio_ * 100}')\n",
    "print(\n",
    "f'Cumulative sum of variance explained for EOF1 and EOF2: {np.cumsum(pca_obj.explained_variance_ratio_) * 100}'\n",
    ")\n",
    "\n",
    "# train kmeans\n",
    "k_means = KMeans(n_clusters=4,\n",
    "                 init='k-means++',\n",
    "                 n_init=10000,\n",
    "                 max_iter=300,\n",
    "                 tol=0.0001,\n",
    "                 verbose=0,\n",
    "                 random_state=0).fit(ds_era5_train)\n",
    "\n",
    "print(f'inertia: {k_means.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0798cc3-fadd-45c2-9f83-240cad8b2d7f",
   "metadata": {},
   "source": [
    "## extract WR indices for use later for bootstrap (significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e4ee64-1436-4ec7-a1d9-ad9b2b495fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grab cluster indices for era5 for bootstrap later\n",
    "\n",
    "z500_era5_boot_1, z500_era5_boot_2, z500_era5_boot_3, z500_era5_boot_4 = cluster_analysis.composite_clusters_indx(\n",
    "    ds_era5_anom.stack(flat=('lat','lon')).transpose('time','flat'), k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb40ef1-1c7a-415e-a35f-610d0ea1f11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab cluster indices for cesm for bootstrap later\n",
    "\n",
    "ds_cesm_anom = narm_analysis.open_cesm_climo_wrs(lat0=10,lat1=70,lon0=-150,lon1=-40)\n",
    "ds_cesm_anom = narm_analysis.cesm_climo_wrs(ds_cesm_anom, rolling_days=5, variable='zg_500')\n",
    "ds_cesm_anom = ds_cesm_anom[get_cold_indx(ds_cesm_anom, mo_init=10, mo_end=3),...]\n",
    "\n",
    "ds_cesm_anom = ds_cesm_anom.sel(lead=slice(0,6)).stack(new=('time','lead'),\n",
    "                                                       flat=('lat','lon')).transpose('new','flat')\n",
    "\n",
    "z500_cesm_boot_1, z500_cesm_boot_2, z500_cesm_boot_3, z500_cesm_boot_4 = cluster_analysis.composite_clusters_indx(\n",
    "    ds_cesm_anom, k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51731e-42c4-444c-9199-d9678b76d8a4",
   "metadata": {},
   "source": [
    "## load hindcast cesm data with lead time bias corrected anomalies (and era5 similarly arranged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ed2b29-43ea-4c0b-b6b0-dab1f3550c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0=10; lat1=70; lon0=-150; lon1=-40\n",
    "\n",
    "# era5 data\n",
    "\n",
    "z500_era5, z500_era5_dt = som_analysis.open_era5_files(\n",
    "    variable='z500', return_time=True, \n",
    "    lat0=lat0,lat1=lat1,lon0=lon0,lon1=lon1,\n",
    "    leadday0=0,leadday1=42,rolldays=5,)\n",
    "\n",
    "z500_standard_era5 = z500_era5.stack(\n",
    "    new=('time','lead'),flat=('lat','lon')).transpose('new','flat')\n",
    "\n",
    "# cesm data\n",
    "\n",
    "z500_cesm, z500_cesm_dt = som_analysis.open_cesm_files(\n",
    "    variable='zg_500', return_time=True, \n",
    "    lat0=lat0,lat1=lat1,lon0=lon0,lon1=lon1,\n",
    "    leadday0=0,leadday1=42,rolldays=5,)\n",
    "\n",
    "z500_standard_cesm = z500_cesm.stack(\n",
    "    new=('time','lead'),flat=('lat','lon')).transpose('new','flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6738dc-be68-49f8-aa7d-d8194434770a",
   "metadata": {},
   "source": [
    "## extract weather regime indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0ed533-9c49-442c-b237-1d4d4069a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grab cluster indices\n",
    "\n",
    "z500_era5_tmp_1, z500_era5_tmp_2, z500_era5_tmp_3, z500_era5_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_era5, k_means, pca_obj, use_pca=True)\n",
    "\n",
    "z500_cesm_tmp_1, z500_cesm_tmp_2, z500_cesm_tmp_3, z500_cesm_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_cesm, k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185a3f2-6f99-4408-ae3d-25ee201f8a7f",
   "metadata": {},
   "source": [
    "## open data across northern hemisphere for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2157a2c-0493-4cdb-8d1f-f97e2e507601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "z500_era5_tmp, _ = som_analysis.open_era5_files(variable='z500', return_time=True, \n",
    "                                                lat0=10,lat1=90,lon0=-360,lon1=0,\n",
    "                                                leadday0=0,leadday1=42,rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "z500_cesm_tmp, _ = som_analysis.open_cesm_files(variable='zg_500', return_time=True, \n",
    "                                                lat0=10,lat1=90,lon0=-360,lon1=0,\n",
    "                                                leadday0=0,leadday1=42,rolldays=5)\n",
    "\n",
    "# restructure data array\n",
    "z500_standard_era5_tmp = z500_era5_tmp.stack(\n",
    "    new=('time','lead'),flat=('lat','lon')).transpose('new','flat')\n",
    "\n",
    "z500_standard_cesm_tmp = z500_cesm_tmp.stack(\n",
    "    new=('time','lead'),flat=('lat','lon')).transpose('new','flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5993e-d8a8-48b7-ab02-e6b5f955d26c",
   "metadata": {},
   "source": [
    "## extract weather regimes from northern hemisphere data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c351fd-8652-4578-a91a-b0c8ac642b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters using indices\n",
    "\n",
    "z500_era5_tmp_01 = z500_standard_era5_tmp.unstack('flat').transpose('new','lat','lon')[z500_era5_tmp_1, :, :]\n",
    "z500_era5_tmp_02 = z500_standard_era5_tmp.unstack('flat').transpose('new','lat','lon')[z500_era5_tmp_2, :, :]\n",
    "z500_era5_tmp_03 = z500_standard_era5_tmp.unstack('flat').transpose('new','lat','lon')[z500_era5_tmp_3, :, :]\n",
    "z500_era5_tmp_04 = z500_standard_era5_tmp.unstack('flat').transpose('new','lat','lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "z500_cesm_tmp_01 = z500_standard_cesm_tmp.unstack('flat').transpose('new','lat','lon')[z500_cesm_tmp_1, :, :]\n",
    "z500_cesm_tmp_02 = z500_standard_cesm_tmp.unstack('flat').transpose('new','lat','lon')[z500_cesm_tmp_2, :, :]\n",
    "z500_cesm_tmp_03 = z500_standard_cesm_tmp.unstack('flat').transpose('new','lat','lon')[z500_cesm_tmp_3, :, :]\n",
    "z500_cesm_tmp_04 = z500_standard_cesm_tmp.unstack('flat').transpose('new','lat','lon')[z500_cesm_tmp_4, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39eb8c3-c6ba-4496-b817-609464183564",
   "metadata": {},
   "source": [
    "## bootstrap for era5 confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b824a8-fe02-4bf6-98bd-440d76900564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data across NH for bootstrap confidence\n",
    "\n",
    "ds_era5_forboot = narm_analysis.era5_z500(lat0=10,lat1=90,lon0=-360,lon1=0)\n",
    "ds_era5_forboot = narm_analysis.era5_climo_wrs(ds_era5_forboot, rolling_days=5, variable='clim')\n",
    "ds_era5_forboot = ds_era5_forboot[get_cold_indx(ds_era5_forboot, mo_init=10, mo_end=3),...]\n",
    "\n",
    "ds_era5_forboot_lon = ds_era5_forboot.lon.values\n",
    "ds_era5_forboot_lat = ds_era5_forboot.lat.values\n",
    "\n",
    "ds_era5_forboot = ds_era5_forboot.values\n",
    "\n",
    "boot_ = np.zeros((ds_era5_forboot.shape[1],\n",
    "                  ds_era5_forboot.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a871be-070c-4d75-a110-b05cafb82384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_era5_forboot.shape[0]) for i in range(z500_era5_boot_1.shape[0])]\n",
    "    boot_ = np.nanmean(ds_era5_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_era5_forboot_lon),\n",
    "            lat=([\"lat\"], ds_era5_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/era5_wr1/z500_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2264f003-83cd-4f72-8652-fa11327262ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_era5_forboot.shape[0]) for i in range(z500_era5_boot_2.shape[0])]\n",
    "    boot_ = np.nanmean(ds_era5_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_era5_forboot_lon),\n",
    "            lat=([\"lat\"], ds_era5_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/era5_wr2/z500_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8596027-47ed-4bd3-ade4-d1254cc42d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_era5_forboot.shape[0]) for i in range(z500_era5_boot_3.shape[0])]\n",
    "    boot_ = np.nanmean(ds_era5_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_era5_forboot_lon),\n",
    "            lat=([\"lat\"], ds_era5_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/era5_wr3/z500_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48232005-27bb-44a8-9a44-b9a65092eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_era5_forboot.shape[0]) for i in range(z500_era5_boot_4.shape[0])]\n",
    "    boot_ = np.nanmean(ds_era5_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_era5_forboot_lon),\n",
    "            lat=([\"lat\"], ds_era5_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/era5_wr4/z500_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b0006-2390-4a43-b8d5-b2cf44838104",
   "metadata": {},
   "source": [
    "## bootstrap for cesm confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4261fc7-2f3c-4c07-abea-ce659517760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cesm data across NH for bootstrap confidence\n",
    "\n",
    "ds_cesm_forboot = narm_analysis.open_cesm_climo_wrs(lat0=10,lat1=90,lon0=-360,lon1=0)\n",
    "ds_cesm_forboot = narm_analysis.cesm_climo_wrs(ds_cesm_forboot, rolling_days=5, variable='zg_500')\n",
    "ds_cesm_forboot = ds_cesm_forboot[get_cold_indx(ds_cesm_forboot, mo_init=10, mo_end=3),...]\n",
    "\n",
    "ds_cesm_forboot_lon = ds_cesm_forboot.lon.values\n",
    "ds_cesm_forboot_lat = ds_cesm_forboot.lat.values\n",
    "\n",
    "ds_cesm_forboot = ds_cesm_forboot.sel(\n",
    "                    lead=slice(0,6)).stack(new=('time','lead')).transpose('new','lat','lon').values\n",
    "\n",
    "boot_ = np.zeros((ds_cesm_forboot.shape[1],\n",
    "                  ds_cesm_forboot.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "071d0acc-6a24-4ef1-9756-70ac5e50de2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_cesm_forboot.shape[0]) for i in range(z500_cesm_boot_1.shape[0])]\n",
    "    boot_ = np.nanmean(ds_cesm_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_cesm_forboot_lon),\n",
    "            lat=([\"lat\"], ds_cesm_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/cesm_wr1/z500_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3fec040-8817-4576-8d8d-0d05588eb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_cesm_forboot.shape[0]) for i in range(z500_cesm_boot_2.shape[0])]\n",
    "    boot_ = np.nanmean(ds_cesm_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_cesm_forboot_lon),\n",
    "            lat=([\"lat\"], ds_cesm_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/cesm_wr2/z500_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f52492a6-c0a2-44ce-95f5-321dfd8f6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_cesm_forboot.shape[0]) for i in range(z500_cesm_boot_3.shape[0])]\n",
    "    boot_ = np.nanmean(ds_cesm_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_cesm_forboot_lon),\n",
    "            lat=([\"lat\"], ds_cesm_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/cesm_wr3/z500_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7261618-f099-4f09-ba18-a58fa1bcf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,10000):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(ds_cesm_forboot.shape[0]) for i in range(z500_cesm_boot_4.shape[0])]\n",
    "    boot_ = np.nanmean(ds_cesm_forboot[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], ds_cesm_forboot_lon),\n",
    "            lat=([\"lat\"], ds_cesm_forboot_lat),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/cesm_wr4/z500_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205dd71-ea42-4900-8c64-7383e54a54bc",
   "metadata": {},
   "source": [
    "## compute confidence intervals from bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d785955-79c8-400c-8a88-96167e71ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_1 = 0.025\n",
    "lev_2 = 0.975\n",
    "lev_3 = 0.005\n",
    "lev_4 = 0.995\n",
    "\n",
    "tmp_era5_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/era5_wr1/z500_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/era5_wr2/z500_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/era5_wr3/z500_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/era5_wr4/z500_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61021faa-1d18-4dc5-a112-b9320c0616ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_cesm_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/cesm_wr1/z500_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/cesm_wr2/z500_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/cesm_wr3/z500_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/cesm_wr4/z500_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1,lev_2,lev_3,lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8cdff-99fc-4465-9432-23af4560a33f",
   "metadata": {},
   "source": [
    "## save data for figures 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2308345-58a0-40f3-a9ef-d3d59eb8920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5 = xr.Dataset(\n",
    "    \n",
    "    data_vars=dict(\n",
    "        \n",
    "        wr_nums=([\"wr\"],np.array([len(z500_era5_tmp_01.new),\n",
    "                                  len(z500_era5_tmp_02.new),\n",
    "                                  len(z500_era5_tmp_03.new),\n",
    "                                  len(z500_era5_tmp_04.new)])),\n",
    "        \n",
    "        wr1_era5=([\"lat\", \"lon\"], z500_era5_tmp_01.mean('new',skipna=True).values),\n",
    "        wr2_era5=([\"lat\", \"lon\"], z500_era5_tmp_02.mean('new',skipna=True).values),\n",
    "        wr3_era5=([\"lat\", \"lon\"], z500_era5_tmp_03.mean('new',skipna=True).values),\n",
    "        wr4_era5=([\"lat\", \"lon\"], z500_era5_tmp_04.mean('new',skipna=True).values),\n",
    "        \n",
    "        wr1_era5_025=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_975=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_005=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_era5_995=([\"lat\", \"lon\"], tmp_era5_wr1.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr2_era5_025=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_975=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_005=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_era5_995=([\"lat\", \"lon\"], tmp_era5_wr2.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr3_era5_025=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_975=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_005=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_era5_995=([\"lat\", \"lon\"], tmp_era5_wr3.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr4_era5_025=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_975=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_005=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_era5_995=([\"lat\", \"lon\"], tmp_era5_wr4.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "    ),\n",
    "    \n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], z500_era5_tmp_01.lon.values),\n",
    "        lat=([\"lat\"], z500_era5_tmp_01.lat.values),\n",
    "        wr= ([\"wr\"],  np.array([1,2,3,4])),\n",
    "    ),\n",
    "    \n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66f88169-90f4-4926-a3dd-6cac40148c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cesm = xr.Dataset(\n",
    "    \n",
    "    data_vars=dict(\n",
    "        \n",
    "        wr_nums=([\"wr\"],np.array([len(z500_cesm_tmp_01.new),\n",
    "                                  len(z500_cesm_tmp_02.new),\n",
    "                                  len(z500_cesm_tmp_03.new),\n",
    "                                  len(z500_cesm_tmp_04.new)])),\n",
    "        \n",
    "        wr1_cesm=([\"lat\", \"lon\"], z500_cesm_tmp_01.mean('new',skipna=True).values),\n",
    "        wr2_cesm=([\"lat\", \"lon\"], z500_cesm_tmp_02.mean('new',skipna=True).values),\n",
    "        wr3_cesm=([\"lat\", \"lon\"], z500_cesm_tmp_03.mean('new',skipna=True).values),\n",
    "        wr4_cesm=([\"lat\", \"lon\"], z500_cesm_tmp_04.mean('new',skipna=True).values),\n",
    "        \n",
    "        wr1_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr1_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr2_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr2_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr3_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr3_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "        \n",
    "        wr4_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.025)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.975)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.005)['iteration'].transpose('lat','lon').values),\n",
    "        wr4_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(quantile=0.995)['iteration'].transpose('lat','lon').values),\n",
    "    ),\n",
    "    \n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], z500_cesm_tmp_01.lon.values),\n",
    "        lat=([\"lat\"], z500_cesm_tmp_01.lat.values),\n",
    "        wr= ([\"wr\"],  np.array([1,2,3,4])),\n",
    "    ),\n",
    "    \n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c894fbb-8a60-49e9-9275-96b3530199c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_samples = xr.Dataset(\n",
    "    \n",
    "    data_vars=dict(\n",
    "        \n",
    "        era5_wr1=([\"era5wr1\",\"lat\",\"lon\"],z500_era5_tmp_01.values),\n",
    "        era5_wr2=([\"era5wr2\",\"lat\",\"lon\"],z500_era5_tmp_02.values),\n",
    "        era5_wr3=([\"era5wr3\",\"lat\",\"lon\"],z500_era5_tmp_03.values),\n",
    "        era5_wr4=([\"era5wr4\",\"lat\",\"lon\"],z500_era5_tmp_04.values),\n",
    "        \n",
    "        cesm_wr1=([\"cesmwr1\",\"lat\",\"lon\"],z500_cesm_tmp_01.values),\n",
    "        cesm_wr2=([\"cesmwr2\",\"lat\",\"lon\"],z500_cesm_tmp_02.values),\n",
    "        cesm_wr3=([\"cesmwr3\",\"lat\",\"lon\"],z500_cesm_tmp_03.values),\n",
    "        cesm_wr4=([\"cesmwr4\",\"lat\",\"lon\"],z500_cesm_tmp_04.values),\n",
    "    ),\n",
    "    \n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], z500_cesm_tmp_01.lon.values),\n",
    "        lat=([\"lat\"], z500_cesm_tmp_01.lat.values),\n",
    "    ),\n",
    "    \n",
    "    attrs=dict(description=\"Table data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1aab86c9-1e3c-4a60-8777-949ff6d1e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5.to_netcdf('/glade/scratch/molina/s2s/bootstrap/era5_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a490e49a-8f18-4e0b-9684-8c90cb571b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cesm.to_netcdf('/glade/scratch/molina/s2s/bootstrap/cesm_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e17a6374-7ebb-4d98-a173-8b4cee900e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_samples.to_netcdf('/glade/scratch/molina/s2s/bootstrap/data_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aacc44-0271-4990-a10c-dac82ea0edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-myenv-tfgpu]",
   "language": "python",
   "name": "conda-env-miniconda3-myenv-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
