{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46ec794-7190-4127-9c45-19bad6218d33",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d746a1b0-1265-48d9-a1fb-cd9621e6ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cf\n",
    "import shapely.geometry as sgeom\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import xskillscore as xs\n",
    "\n",
    "import som_analysis\n",
    "import cluster_analysis\n",
    "import narm_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0046f-94e8-4e58-bc39-62bf5fa2c426",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c339c36-c2f3-48f3-b4e5-998914deec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_insetmap(axes_extent, map_extent, lons, lats, temp_data, \n",
    "                 vmin, vmax, cmap='coolwarm'):\n",
    "\n",
    "    use_projection = ccrs.Mercator()     # preserve shape well\n",
    "    geodetic = ccrs.Geodetic(globe=ccrs.Globe(datum='WGS84'))\n",
    "\n",
    "    sub_ax = plt.axes(axes_extent, projection=use_projection)  # normal units\n",
    "\n",
    "    sub_ax.set_extent(map_extent, geodetic)  # map extents\n",
    "    sub_ax.coastlines(linewidth=0.35, zorder=10)\n",
    "\n",
    "    sub_ax.pcolormesh(lons, lats, temp_data, transform=ccrs.PlateCarree(),\n",
    "                      vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "    extent_box = sgeom.box(\n",
    "        map_extent[0], map_extent[2], map_extent[1], map_extent[3])\n",
    "    sub_ax.add_geometries(\n",
    "        [extent_box], ccrs.PlateCarree(), color='none', linewidth=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e98c69-af7c-4829-bca6-988812581d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_indx(ds, mo_init=9, mo_end=2):\n",
    "    \"\"\"\n",
    "    Extract indices for cold season.\n",
    "    Grabbing Sept thru February init, for Oct thru March predictions.\n",
    "    \"\"\"\n",
    "    dt_array = pd.to_datetime(ds['time'])\n",
    "    return xr.where((dt_array.month >= mo_init) | (\n",
    "        dt_array.month <= mo_end), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eccf52-3d6d-470a-b583-1a941401e5da",
   "metadata": {},
   "source": [
    "## open and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd90e72-c9cf-4fd3-92aa-f941b8b7f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region for clustering\n",
    "lat0 = 10\n",
    "lat1 = 70\n",
    "lon0 = -150\n",
    "lon1 = -40\n",
    "\n",
    "# open era5 data and slice\n",
    "ds_era5 = narm_analysis.era5_z500(lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1)\n",
    "\n",
    "# era5 anomalies\n",
    "ds_era5_anom = narm_analysis.era5_climo_wrs(\n",
    "    ds_era5, rolling_days=5, variable='clim')\n",
    "\n",
    "# restructure era5 array for machine learning training (SONDJFM)\n",
    "ds_era5_anom = ds_era5_anom[get_cold_indx(\n",
    "    ds_era5_anom, mo_init=10, mo_end=3), ...]\n",
    "ds_era5_train = ds_era5_anom.stack(\n",
    "    flat=('lat', 'lon')).transpose('time', 'flat').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a09f-1ef8-4356-94d5-06998421187c",
   "metadata": {},
   "source": [
    "## pca and kmeans with era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b625fee7-0663-4615-abd6-d455765106cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: [25.95315607 17.65410568 11.94871708  9.0784389   7.98100848  6.14181738\n",
      "  4.32605934  2.61658689  2.22642929  2.17049559  1.49813958  1.22541708]\n",
      "Cumulative sum of variance explained for EOF1 and EOF2: [25.95315607 43.60726175 55.55597883 64.63441774 72.61542622 78.7572436\n",
      " 83.08330294 85.69988983 87.92631912 90.09681471 91.59495429 92.82037136]\n",
      "inertia: 39379.205381011976\n"
     ]
    }
   ],
   "source": [
    "# create pca object\n",
    "pca_obj = PCA(12, whiten=True)\n",
    "\n",
    "# fit pca with era5\n",
    "pca_obj = pca_obj.fit(ds_era5_train)\n",
    "\n",
    "# transform era5 data with pca\n",
    "ds_era5_train = pca_obj.transform(ds_era5_train)\n",
    "\n",
    "print(f'Variance explained: {pca_obj.explained_variance_ratio_ * 100}')\n",
    "print(\n",
    "f'Cumulative sum of variance explained for EOF1 and EOF2: {np.cumsum(pca_obj.explained_variance_ratio_) * 100}'\n",
    ")\n",
    "\n",
    "# train kmeans\n",
    "k_means = KMeans(n_clusters=4,\n",
    "                 init='k-means++',\n",
    "                 n_init=10000,\n",
    "                 max_iter=300,\n",
    "                 tol=0.0001,\n",
    "                 verbose=0,\n",
    "                 random_state=0).fit(ds_era5_train)\n",
    "\n",
    "print(f'inertia: {k_means.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51731e-42c4-444c-9199-d9678b76d8a4",
   "metadata": {},
   "source": [
    "## load data with lead time bias corrected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ed2b29-43ea-4c0b-b6b0-dab1f3550c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 data\n",
    "z500_era5, z500_era5_dt = som_analysis.open_era5_files(\n",
    "    variable='z500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# cesm data\n",
    "z500_cesm, z500_cesm_dt = som_analysis.open_cesm_files(\n",
    "    variable='zg_500', return_time=True,\n",
    "    lat0=lat0, lat1=lat1, lon0=lon0, lon1=lon1,\n",
    "    leadday0=0, leadday1=42, rolldays=5)\n",
    "\n",
    "# restructure arrays\n",
    "z500_standard_era5 = z500_era5.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')\n",
    "z500_standard_cesm = z500_cesm.stack(\n",
    "    new=('time', 'lead'), flat=('lat', 'lon')).transpose('new', 'flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67582b5-a219-4d7d-a4d4-c82f4217a777",
   "metadata": {},
   "source": [
    "## composites of the weather types/regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0ed533-9c49-442c-b237-1d4d4069a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab cluster indices\n",
    "\n",
    "z500_era5_tmp_1, z500_era5_tmp_2, z500_era5_tmp_3, z500_era5_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_era5, k_means, pca_obj, use_pca=True)\n",
    "\n",
    "z500_cesm_tmp_1, z500_cesm_tmp_2, z500_cesm_tmp_3, z500_cesm_tmp_4 = cluster_analysis.composite_clusters_indx(\n",
    "    z500_standard_cesm, k_means, pca_obj, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990b7-f326-4657-9895-163b60af3510",
   "metadata": {
    "tags": []
   },
   "source": [
    "## temperature anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa163bf-cea7-4793-a3b6-72b0828a9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0_tmp = 10\n",
    "lat1_tmp = 75\n",
    "lon0_tmp = -165\n",
    "lon1_tmp = -40\n",
    "\n",
    "# temperature\n",
    "\n",
    "# noaa data\n",
    "t2m_noaa, _ = som_analysis.open_noaa_files(\n",
    "    variable='temp', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "mask = xr.where(~np.isnan(t2m_noaa.isel(time=0, lead=0)), 1.0, np.nan)\n",
    "\n",
    "# era5 data\n",
    "t2m_era5, _ = som_analysis.open_era5_files(\n",
    "    variable='temp', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "t2m_era5 = t2m_era5.where(mask == 1.0)\n",
    "\n",
    "# cesm data\n",
    "t2m_cesm, _ = som_analysis.open_cesm_files(\n",
    "    variable='tas_2m', return_time=True,\n",
    "    lat0=lat0_tmp, lat1=lat1_tmp, lon0=lon0_tmp, lon1=lon1_tmp,\n",
    "    leadday0=0, leadday1=42, rolldays=1,)\n",
    "\n",
    "t2m_cesm = t2m_cesm.where(mask == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dcab496-9ae3-4d17-9c49-a43a2e53a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_noaa_01 = t2m_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "t2m_noaa_02 = t2m_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "t2m_noaa_03 = t2m_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "t2m_noaa_04 = t2m_noaa.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "t2m_era5_01 = t2m_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_1, :, :]\n",
    "t2m_era5_02 = t2m_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_2, :, :]\n",
    "t2m_era5_03 = t2m_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_3, :, :]\n",
    "t2m_era5_04 = t2m_era5.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_era5_tmp_4, :, :]\n",
    "\n",
    "t2m_cesm_01 = t2m_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_1, :, :]\n",
    "t2m_cesm_02 = t2m_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_2, :, :]\n",
    "t2m_cesm_03 = t2m_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_3, :, :]\n",
    "t2m_cesm_04 = t2m_cesm.stack(new=('time', 'lead')).transpose(\n",
    "    'new', 'lat', 'lon')[z500_cesm_tmp_4, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bac74-f4a7-449e-b9fd-c72232e8af88",
   "metadata": {},
   "source": [
    "## bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69dafafe-4b55-4210-ab61-f403950d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday = 1\n",
    "seconday = 14\n",
    "\n",
    "lons = t2m_noaa_01.lon.values\n",
    "lats = t2m_noaa_01.lat.values\n",
    "\n",
    "boot_num_init_ = 1000\n",
    "boot_num_iter_ = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cc92a-9618-41f2-9019-e029d81ed3f4",
   "metadata": {},
   "source": [
    "## noaa bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5e72f-634a-4245-b1fa-ae9c548feae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = t2m_noaa.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633d53b-2c7c-4bfa-8583-de6c0720290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = t2m_noaa_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr1_week12/t2m_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d55bb6b-6c03-4928-8ba3-aa4f2fe55980",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = t2m_noaa_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(7200, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr2_week12/t2m_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aebe0166-1fbe-45c9-8a35-661af4ba6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_189713/2060033009.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_noaa_03.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr3_week12/t2m_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdd0a5d-429b-43f8-9e35-9962c2c4eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_189713/4291095622.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_noaa_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr4_week12/t2m_ncpc_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa2d62-615b-48c2-ad65-a061b5aede82",
   "metadata": {},
   "source": [
    "## era5 bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630d2c3f-2b6c-48b0-971c-17139344f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = t2m_era5.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80aec85a-6470-439e-85dc-a53bbc60ed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_189713/1541693190.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_era5_01.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr1_week12/t2m_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf181c9-2d30-4caf-8f3f-ebd52045aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_189713/582870324.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_era5_02.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr2_week12/t2m_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa30fc8-8e1e-4c52-a003-9804f1f499a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_136319/359358041.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_era5_03.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(5702, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr3_week12/t2m_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6a793-ae6a-498a-898b-4ab866676f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_136319/2611253228.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_era5_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr4_week12/t2m_era5_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36e1cb-7390-4e67-8c04-cc15dc40fc23",
   "metadata": {},
   "source": [
    "## cesm bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfead661-aae4-47ab-bff2-eaec4bc5bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_all = t2m_cesm.isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('time', 'lead')).transpose('new', 'lat', 'lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf1114-187a-43a6-970b-8dc6f977a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = t2m_cesm_01.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr1_week12/t2m_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5dd5f-1a70-46eb-8ed1-3558b1647f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = t2m_cesm_02.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr2_week12/t2m_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87b32-f69f-4ce5-ac4c-bcad040090b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = t2m_cesm_03.unstack('new').isel(lead=slice(firstday,seconday)).stack(\n",
    "    new=('lead','time')).transpose('new','lat','lon').values\n",
    "\n",
    "for ind in range(boot_num_init_, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx,...], axis=0)\n",
    "    \n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr3_week12/t2m_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3bf3c5-928e-4814-bf86-09769dd0e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/molina/ipykernel_200178/2049057487.py:8: RuntimeWarning: Mean of empty slice\n",
      "  boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n"
     ]
    }
   ],
   "source": [
    "tmp_data = t2m_cesm_04.unstack('new').isel(lead=slice(firstday, seconday)).stack(\n",
    "    new=('lead', 'time')).transpose('new', 'lat', 'lon').values\n",
    "\n",
    "for ind in range(3590, boot_num_iter_):\n",
    "\n",
    "    np.random.seed(ind + 1)\n",
    "    rand_indx = [np.random.choice(tmp_all.shape[0]) for i in range(tmp_data.shape[0])]\n",
    "    boot_ = np.nanmean(tmp_all[rand_indx, ...], axis=0)\n",
    "\n",
    "    xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            iteration=([\"lat\", \"lon\"], boot_),\n",
    "        ),\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], lons),\n",
    "            lat=([\"lat\"], lats),\n",
    "        ),\n",
    "        attrs=dict(description=\"For bootstrap confidence intervals.\"),\n",
    "    ).to_netcdf(\n",
    "        f'/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr4_week12/t2m_cesm_boot_{ind + 1}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfec2d-2785-4c12-b411-087ac8e9008e",
   "metadata": {},
   "source": [
    "## compute bootstrap percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29b7a3e-4729-4add-ab0b-0828e6120011",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_1 = 0.025\n",
    "lev_2 = 0.975\n",
    "lev_3 = 0.005\n",
    "lev_4 = 0.995\n",
    "\n",
    "# ncpc\n",
    "\n",
    "tmp_ncpc_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr1_week12/t2m_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr2_week12/t2m_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr3_week12/t2m_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_ncpc_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_ncpc_wr4_week12/t2m_ncpc_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# era5\n",
    "\n",
    "tmp_era5_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr1_week12/t2m_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr2_week12/t2m_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr3_week12/t2m_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_era5_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_era5_wr4_week12/t2m_era5_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "# cesm\n",
    "\n",
    "tmp_cesm_wr1 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr1_week12/t2m_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr2 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr2_week12/t2m_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr3 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr3_week12/t2m_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)\n",
    "\n",
    "tmp_cesm_wr4 = xr.open_mfdataset(\n",
    "    '/glade/scratch/molina/s2s/bootstrap/t2m_cesm_wr4_week12/t2m_cesm_boot_*.nc',\n",
    "    combine='nested', concat_dim='iter').chunk(\n",
    "    dict(iter=-1)).quantile([lev_1, lev_2, lev_3, lev_4], dim='iter', skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5abed-c37b-4b0e-a7b0-fdf6b8cf5996",
   "metadata": {},
   "source": [
    "## assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bcda312-b652-44c2-82cb-4c86c2924b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "/glade/work/molina/miniconda3/envs/myenv-tfgpu/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1395: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    }
   ],
   "source": [
    "ds_t2m = xr.Dataset(\n",
    "\n",
    "    data_vars=dict(\n",
    "\n",
    "        wr1_ncpc=([\"lat\", \"lon\"], t2m_noaa_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_ncpc=([\"lat\", \"lon\"], t2m_noaa_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_ncpc=([\"lat\", \"lon\"], t2m_noaa_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_ncpc=([\"lat\", \"lon\"], t2m_noaa_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_ncpc_025=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_975=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_005=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_ncpc_995=([\"lat\", \"lon\"], tmp_ncpc_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr1_era5=([\"lat\", \"lon\"], t2m_era5_01.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_era5=([\"lat\", \"lon\"], t2m_era5_02.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_era5=([\"lat\", \"lon\"], t2m_era5_03.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_era5=([\"lat\", \"lon\"], t2m_era5_04.unstack('new').isel(\n",
    "            lead=slice(firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_era5_025=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_975=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_005=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_era5_995=([\"lat\", \"lon\"], tmp_era5_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_era5_025=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_975=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_005=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_era5_995=([\"lat\", \"lon\"], tmp_era5_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_era5_025=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_975=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_005=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_era5_995=([\"lat\", \"lon\"], tmp_era5_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_era5_025=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_975=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_005=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_era5_995=([\"lat\", \"lon\"], tmp_era5_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr1_cesm=([\"lat\", \"lon\"], t2m_cesm_01.unstack('new').isel(lead=slice(\n",
    "            firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr2_cesm=([\"lat\", \"lon\"], t2m_cesm_02.unstack('new').isel(lead=slice(\n",
    "            firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr3_cesm=([\"lat\", \"lon\"], t2m_cesm_03.unstack('new').isel(lead=slice(\n",
    "            firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "        wr4_cesm=([\"lat\", \"lon\"], t2m_cesm_04.unstack('new').isel(lead=slice(\n",
    "            firstday, seconday)).stack(new=('lead', 'time')).mean(\n",
    "            'new', skipna=True).where(mask == 1.0).values),\n",
    "\n",
    "        wr1_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr1_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr1.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr2_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr2_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr2.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr3_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr3_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr3.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "\n",
    "        wr4_cesm_025=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.025)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_975=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.975)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_005=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.005)['iteration'].transpose('lat', 'lon').values),\n",
    "        wr4_cesm_995=([\"lat\", \"lon\"], tmp_cesm_wr4.sel(\n",
    "            quantile=0.995)['iteration'].transpose('lat', 'lon').values),\n",
    "    ),\n",
    "\n",
    "    coords=dict(\n",
    "        lon=([\"lon\"], lons),\n",
    "        lat=([\"lat\"], lats),\n",
    "    ),\n",
    "\n",
    "    attrs=dict(description=\"Figure data for weather regimes research.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1764e7b-d099-49a7-8589-a7e824a18ded",
   "metadata": {},
   "source": [
    "## save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cdb5a3-7844-41d3-b886-8c73e2983c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t2m.to_netcdf('/glade/scratch/molina/s2s/bootstrap/t2m_week12_wxregimes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab467ef9-95d0-402a-9eff-d2d4f43cd197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9253d7-f7b4-4fdd-9f34-1a0b7188011d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-myenv-tfgpu]",
   "language": "python",
   "name": "conda-env-miniconda3-myenv-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
